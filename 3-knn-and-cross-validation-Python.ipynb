{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hess Data Set \n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine\n",
    "from plotnine import ggplot, aes, facet_wrap, geom_point, stat_smooth, scale_x_log10\n",
    "plotnine.theme_set(plotnine.theme_bw())\n",
    "import sklearn as sk\n",
    "\n",
    "from maclearn_utils_2020 import extractPCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hess data set I initially mentioned in the introductory notes\n",
    "consists of microarray data taken from fine-needle biopsies taken from\n",
    "breast cancer patients. A number of patient characteristics were\n",
    "collected, but the main focus of the modeling that we will be\n",
    "doing---like the modeling that Hess et al. were doing!\n",
    "([@hess2006pharmacogenomic])---will be the sensivity to\n",
    "preoperative chemotherapy, with the patients divided into those who\n",
    "exhibited residual disease (RD) or those who did not and were thus\n",
    "classified as have pathologic complete response (pCR) to chemotherapy.\n",
    "In order to load the Hess data in, let's re-define the function `rt`\n",
    "we used to load the Neves data in before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## define convenience function for loading tabular data\n",
    " ## (just using read.table with different default options)\n",
    "def rt(f):\n",
    "    return pd.read_csv(f, sep=\"\\t\", index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hess et al. obtained two separate data sets, a training set which they\n",
    "used to develop a classifier for RD-vs-pCR, and a test set which they\n",
    "used to assess the performance of the resulting classifier. Let's load\n",
    "in the training data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## training set:\n",
    "hessTrain = rt(\"data/HessTrainingData.tsv.gz\")\n",
    "hessTrainAnnot = rt(\"data/HessTrainingAnnotation.tsv\")\n",
    " ## align annotation data.frame with expression data:\n",
    "hessTrainAnnot = hessTrainAnnot.loc[hessTrain.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## test set:\n",
    "hessTest = rt(\"data/HessTestData.tsv.gz\")\n",
    "hessTestAnnot = rt(\"data/HessTestAnnotation.tsv\")\n",
    " ## align annotation data.frame with expression data:\n",
    "hessTestAnnot = hessTestAnnot.loc[hessTest.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessTrain.iloc[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the rows here are not annotated by gene ids but instead by\n",
    "*probe set* ids. We'll load in the microarray probe annotations\n",
    "mapping these probe sets back to genes as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probeAnnot = rt(\"data/U133A.tsv.gz\")\n",
    " ## align hessTrain and hessTest to probeAnnot:\n",
    "hessTrain = hessTrain.loc[probeAnnot.index]\n",
    "hessTest = hessTest.loc[probeAnnot.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of classification using `sklearn`, we'd like to extract the\n",
    "class labels from columns of the sample annotation files into\n",
    "0-1 numeric `pd.Series` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessTrainY = pd.Series({\"pCR\":0, \"RD\":1}).loc[hessTrainAnnot[\"pCRtxt\"]]\n",
    "hessTrainY.index = hessTrainAnnot.index\n",
    "hessTestY = pd.Series({\"pCR\":0, \"RD\":1}).loc[hessTestAnnot[\"pCRtxt\"]]\n",
    "hessTestY.index = hessTestAnnot.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a quick look at the test and training set data put\n",
    "together via a PCA plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## use pd.concat to column-bind the training and test data sets together:\n",
    "combinedData = pd.concat([hessTrain, hessTest], axis=1)\n",
    "pca = extractPCs(combinedData.T)\n",
    "ggdata = pd.DataFrame({\n",
    "    \"PC1\" : pca.x[:, 0],\n",
    "    \"PC2\" : pca.x[:, 1],\n",
    "    \"set\" : [\"train\"]*len(hessTrainY) + [\"test\"]*len(hessTestY)\n",
    "})\n",
    "gg = ggplot(ggdata, aes(x=\"PC1\", y=\"PC2\", color=\"set\"))\n",
    "gg += geom_point(size=2)\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows an uncomfortable fact about real-world applications of\n",
    "machine learning: when training and test sets are collected\n",
    "separately, there tend to be systematic differences between them. This\n",
    "can result in degraded test-set performance even when models have been\n",
    "carefully constructed using the most effective algorithms available!\n",
    "\n",
    "$k$-Nearest Neighbors (knn) \n",
    "=========================================\n",
    "\n",
    "The $k$-neighest neighbors, or knn, algorithm\n",
    "([@cover1968estimation]) is a particularly simple and democratic\n",
    "approach to classification:\n",
    "To classify sampling unit $i$ with feature values $x_ig$:\n",
    "-   find the $k$ sampling units $\\{j_1, j_2, \\ldots, j_k\\}$ from the\n",
    "    training set *most similar* to $i$: these are the \"nearest\n",
    "    neighbors\"\n",
    "-   calculate the fraction $\\frac{\\sum_b y_{j_b}}{k}$ of the nearest\n",
    "    neighbors which have $y_{j_{b}} = 1$: this is the knn model-predicted probability\n",
    "    that $y_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## extract vector of gene expression values for test sample 1:\n",
    "featuresForTestSample1 = hessTest.iloc[:, 0]\n",
    " ## calculate distance of each training sample from first test sample:\n",
    " ## note: subtraction of vector from matrix is row-wise in Python!\n",
    "euclideanDistancesFromTrainSamples = np.sqrt(\n",
    "    ( (hessTrain.T - featuresForTestSample1)**2 ).sum(axis=1)\n",
    ")\n",
    " ## what are the 9 nearest neighbors\n",
    " ## and their distances from first test sample?\n",
    "nn = euclideanDistancesFromTrainSamples.sort_values()[0:9]\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## what are their classifications?\n",
    "hessTrainY.loc[nn.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## 9-nn model predicted probability of RD for first test sample is:\n",
    "np.sum( hessTrainY.loc[nn.index] ) / 9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I should hasten to point out that all of the ML algorithms we\n",
    "will study here have been been packaged up into more efficient and\n",
    "user-friendly Python routines, so there is really no need to go\n",
    "through the pain of re-implementing them from scratch! (I just wanted\n",
    "to give you a sense of how simple knn in particular is ``under the\n",
    "hood.'')\n",
    "\n",
    "Here is the way I would actually suggest to apply the knn algorithm in\n",
    "Python (using `sklearn.neighbors.KNeighborsClassifier`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as nbr\n",
    " ## fit model object obtained by running\n",
    " ## (remembering sklearn wants features-in-columns):\n",
    "knnFit = nbr.KNeighborsClassifier(n_neighbors=9)\\\n",
    "            .fit(hessTrain.T, hessTrainY)\n",
    " ## can then generate test set predictions using knnFit:\n",
    "knnTestPredictionProbs = knnFit.predict_proba(hessTest.T)\n",
    " ## sklearn predict_proba will always produce matrix:\n",
    "knnTestPredictionProbs[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## what is predicted probability RD for first test sample again?\n",
    "knnTestPredictionProbs[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sklearn, probabilitistic prediction is done using `predict_proba`,\n",
    "while predicted classifications may be computed using the plain\n",
    "`predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnTestPredictionClass = knnFit.predict(hessTest.T)\n",
    "knnTestPredictionClass[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## use pd.crosstab to generate 2x2 contingency table:\n",
    "contingency = pd.crosstab(knnTestPredictionClass, hessTestY,\n",
    "                          rownames=[\"prediction\"], colnames=[\"actual\"])\n",
    "contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2x2 contingency table is a very useful and commonly presented\n",
    "summary of binary classification results. If one class is regarded as\n",
    "\"positive\" and one as \"negative,\" the various cells of the 2x2\n",
    "table can be labeled:\n",
    "\n",
    "|                 | Actual (-)             | Actual (+)            |\n",
    "|-----------------|------------------------|-----------------------|\n",
    "| Predicted (-)   | True Negatives (TN)    | False Negatives (FN)  |\n",
    "| Predicted (+)   | False Positives (FP)   | True Positives (TP)   |\n",
    "\n",
    "Notice that:\n",
    "-   the diagonal elements of the contingency table correspond to\n",
    "    accurate classifications, and that\n",
    "-   every (classifiable) sampling unit will fall into one of the\n",
    "    four cells.\n",
    "\n",
    "Thus we can calculate the fraction of sampling units classified\n",
    "correctly---referred to in ML contexts as the *accuracy* of the\n",
    "model fit---by dividing the sum of the diagonals of the contingency\n",
    "table by the sum of all four entries in the contingency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatedAccuracy = np.sum(np.diag(contingency)) / np.sum(np.sum(contingency))\n",
    "estimatedAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting \n",
    "===========\n",
    "\n",
    "Let's use the data for two specific microarray probes,\n",
    "205548_s_at and 201976_s_at to fit a knn model with $k=27$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## we'll go ahead and transpose the data.frame to have\n",
    " ## features-in-columns for convenience:\n",
    "twoProbeData = hessTrain.T.loc[:, [\"205548_s_at\", \"201976_s_at\"]].copy()\n",
    " ## let's use friendlier gene names instead of probe ids here:\n",
    "twoProbeData.columns =\\\n",
    "        probeAnnot.loc[twoProbeData.columns, \"Gene.Symbol\"]\n",
    "twoProbeFitK27 = nbr.KNeighborsClassifier(n_neighbors=27)\\\n",
    "                    .fit(twoProbeData, hessTrainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using two probes because I want to be able to make a contour plot\n",
    "of the *decision boundary* of the knn classifier. I'm gonig to\n",
    "use a function saved in the file `ggfuntile.py` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ggfuntile import predictionContour\n",
    " ## unfortunately plotnine doesn't support geom_contour,\n",
    " ## so Python predictionContour is somewhat misnamed!\n",
    "predictionContour(twoProbeFitK27, twoProbeData, hessTrainY, \"k = 27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this decision boundary you might think this classifier\n",
    "looks too conservative about calling pCR: Surely we could push that\n",
    "boundary to the left a bit to catch a few more of those open\n",
    "downward-pointing triangles? Thinking about this a bit more, it seems\n",
    "that perhaps our choice of parameter $k=27$ is a bit high; after all,\n",
    "27 is almost a third of all 82 samples in the\n",
    "Hess training set. The appropriate neighborhood for points in the\n",
    "upper right hand corner of the contour plot may be better estimated\n",
    "with a more local knn model defined by, say, $k=9$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoProbeFitK9 = nbr.KNeighborsClassifier(n_neighbors=9)\\\n",
    "                   .fit(twoProbeData, hessTrainY)\n",
    "predictionContour(twoProbeFitK9, twoProbeData, hessTrainY, \"k = 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does look somewhat better! Many more pCR samples correctly called\n",
    "at the cost of only one extra misclassified RD sample. But perhaps we\n",
    "could do better still with an even more local model---let's try $k=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoProbeFitK3 = nbr.KNeighborsClassifier(n_neighbors=3)\\\n",
    "                   .fit(twoProbeData, hessTrainY)\n",
    "predictionContour(twoProbeFitK3, twoProbeData, hessTrainY, \"k = 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm\\...this does appear to catch a few more pCR samples from the\n",
    "training set, but we seem to have generated some swiss cheese-like\n",
    "holes in the RD predicted region, along with a very convoluted bay\n",
    "and peninsula in the center right portion of the main decision\n",
    "boundary. Still, this seems like a very subjective complaint---let's\n",
    "look at some accuracy estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## define pair of convenience functions to minimize repeated code:\n",
    "def contingencize(knnFit, data, y):\n",
    "    return pd.crosstab(knnFit.predict(data), y,\n",
    "                       rownames=[\"prediction\"], colnames=[\"actual\"])\n",
    "\n",
    "def estimateAccuracyFrom2x2(twoByTwo):\n",
    "    return np.sum(np.diag(twoByTwo)) / np.sum(np.sum(twoByTwo))\n",
    "\n",
    "twoByTwo27 = contingencize(twoProbeFitK27, twoProbeData, hessTrainY)\n",
    "estimateAccuracyFrom2x2(twoByTwo27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoByTwo9 = contingencize(twoProbeFitK9, twoProbeData, hessTrainY)\n",
    "estimateAccuracyFrom2x2(twoByTwo9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoByTwo3 = contingencize(twoProbeFitK3, twoProbeData, hessTrainY)\n",
    "estimateAccuracyFrom2x2(twoByTwo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So does this really mean the swiss-cheese decision region is the\n",
    "best\\...?\n",
    "\n",
    "Of course not! All of the accuracy estimates we just made suffer from\n",
    "what's called *resubstitution bias* because we tested the model\n",
    "on the same data set that was used to train it. Let's clean that up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## extract test data for our two favorite probes...\n",
    "twoProbeTest = hessTest.T.loc[:, [\"205548_s_at\", \"201976_s_at\"]]\n",
    "twoProbeTest.columns =\\\n",
    "        probeAnnot.loc[twoProbeTest.columns, \"Gene.Symbol\"]\n",
    " ## now let's take another stab at accuracy estimations:\n",
    "twoByTwo27 = contingencize(twoProbeFitK27, twoProbeTest, hessTestY)\n",
    "estimateAccuracyFrom2x2(twoByTwo27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoByTwo9 = contingencize(twoProbeFitK9, twoProbeTest, hessTestY)\n",
    "estimateAccuracyFrom2x2(twoByTwo9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoByTwo3 = contingencize(twoProbeFitK3, twoProbeTest, hessTestY)\n",
    "estimateAccuracyFrom2x2(twoByTwo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we may be a bit disappointed to see that the best accuracy\n",
    "estimate from the test set is worse than the worst accuracy estimate\n",
    "from resubstitution of the training set, we can find solace in noting\n",
    "that the $k=3$ model with it's bizarre decision boundary is no longer\n",
    "judged the best.\n",
    "\n",
    "This is the classic problem of *overfitting*: Models with more\n",
    "freedom to fit very complex patterns in the training data set---such\n",
    "as our very local low-$k$ knn model---have a tendency to find\n",
    "\"signals\" which are not reproducible in independent data sets, even\n",
    "those of a very similar nature.\n",
    "\n",
    "Here's an example where you can see the overfitting coming without any\n",
    "computation at all: What do you think the resubstitution accuracy of a\n",
    "1-nearest neighbor model would be? As a hint, you might think about what\n",
    "the nearest neighbor of training sample $i$ is in the training set\\...\n",
    "\n",
    "knn Simulation \n",
    "==============\n",
    "\n",
    "At this point I will digress away from analysis of the Hess microarray\n",
    "data for a bit to consider simulated data sets. Simulated data can be\n",
    "useful because:\n",
    "1.  we know the true model used to generate the data exactly, and\n",
    "2.  we can systematically vary any parameters that appear in the\n",
    "    data generation model so as to study how well our ML algorithms work\n",
    "    in a range of situations.\n",
    "\n",
    "Let's define a function `simulate2group` for simulating\n",
    "-   a simple data set with Rn sampling units (or simulated samples)\n",
    "    and `m` features (simulated genes, if you like),\n",
    "-   with the sampling units divided into two groups A and B,\n",
    "-   and `mEffected` $\\leq$ `m` of the features being shifted by\n",
    "    `effectSize` units on average in group B relative to group A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate2group(n = 100,   ## number simulated samples\n",
    "                   m = 1000,  ## number simulated genes\n",
    "                   nA = None, ## first nA samples = group A\n",
    "                              ## last (n-nA) samples = group B\n",
    "                   mEffected = 10,    ## first mEffected genes will have\n",
    "                                      ## different expression in group B\n",
    "                   effectSize = 1 ## how many expression units difference\n",
    "                                  ## between groups for mEffected genes\n",
    "                   ):\n",
    "    if nA is None:\n",
    "        nA = int(np.ceil(0.5*n))\n",
    "     ## in case numpy int types have been passed in as arguments:\n",
    "    n=int(n); m=int(m); nA=int(nA); mEffected=int(mEffected)\n",
    "    x = pd.DataFrame(np.random.randn(n, m))   ## simulate iid expression values\n",
    "                                              ## (highly unrealistic, but easy)\n",
    "    y = pd.Series([0]*nA + [1]*(n-nA))\n",
    "    x.columns = \"g\" + pd.Series(range(m)).astype(str) ## gene labels g1, g2, ...\n",
    "    x.index = \"i\" + pd.Series(range(n)).astype(str)   ## sample labels i1, i2, ...\n",
    "    y.index = x.index\n",
    "    x.loc[y == 1, x.columns[range(mEffected)]] =\\\n",
    "            x.loc[y == 1, x.columns[range(mEffected)]] + effectSize\n",
    "    return {\"x\":x, \"y\":y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the second advantage associated with simulated data\n",
    "above---the ability to repeat the analysis while varying simulation\n",
    "parameters---I'm going to package our data generation, model fitting,\n",
    "and model assessment procedure up into a function of those simulation\n",
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateAndKnnModel(n, m, k, mEffected, effectSize, rep=1, **kwargs):\n",
    "     ## in case numpy int types have been passed in as arguments:\n",
    "    n=int(n); m=int(m); k=int(k); mEffected=int(mEffected); rep=int(rep)\n",
    "    trainSet = simulate2group(n, m, mEffected=mEffected, effectSize=effectSize)\n",
    "    testSet = simulate2group(n, m, mEffected=mEffected, effectSize=effectSize)\n",
    "    knnFit = nbr.KNeighborsClassifier(n_neighbors=k)\\\n",
    "                .fit(trainSet[\"x\"], trainSet[\"y\"])\n",
    "    resubstitutionPredictions = knnFit.predict(trainSet[\"x\"])\n",
    "     ## use pd.crosstab function to make 2x2 contingency table of results\n",
    "    resub2by2 = pd.crosstab(resubstitutionPredictions, trainSet[\"y\"])\n",
    "     ## diagonals of 2x2 are number true pos, true neg;\n",
    "     ## off-diagonals give numbers of false pos, false neg, so:\n",
    "    resubAccuracyEst = np.sum(np.diag(resub2by2)) / np.sum(np.sum(resub2by2))\n",
    "     ## do same thing for testPredictions:\n",
    "    testPredictions = knnFit.predict(testSet[\"x\"])\n",
    "    test2by2 = pd.crosstab(testPredictions, testSet[\"y\"])\n",
    "    testAccuracyEst = np.sum(np.diag(test2by2)) / np.sum(np.sum(test2by2))\n",
    "     ## return vector of results along with simulation parameters:\n",
    "    return(pd.Series({\"m\" : m,\n",
    "                      \"k\" : k,\n",
    "                      \"rep\" : rep,\n",
    "                      \"resubstitution\" : resubAccuracyEst,\n",
    "                      \"test\" : testAccuracyEst}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example using this function to assess the performance of a\n",
    "5-nearest neighbors model (`k=5`) on a simulated data set of\n",
    "`n=100` sampling units with `m=10` features, of which\n",
    "`mEffected=1` feature has values elevated by `effectSize=2.5`\n",
    "units in group B relative to group A (we'll rely on the\n",
    "`simulate2group` default value of `nA=ceiling(0.5*n)=50` to\n",
    "specify that half of the sampling units are in group A and the other\n",
    "half in group B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateAndKnnModel(n=100, m=10, k=5, mEffected=1, effectSize=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function reports out some of the simulation parameters along with\n",
    "the estimated accuracy results so that we can keep track of what\n",
    "parameters went into eah simulation when we repeat this procedure many\n",
    "times. We're going to do this by setting up a `DataFrame` with one\n",
    "row per simulation and columns specifying the parameters to use;\n",
    "first we'll define a convenience function `expandGrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## expandGrid generates data.frame with all combinations of\n",
    " ## supplied arguments\n",
    "def expandGrid(od):\n",
    "    cartProd = list(itertools.product(*od.values()))\n",
    "    return pd.DataFrame(cartProd, columns=od.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now on to using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## package desired parameter values into ordered dictionary\n",
    " ## (keys = parameter names, values = lists of parameter values)\n",
    "parameterValues = OrderedDict()\n",
    "parameterValues[\"n\"] = [100]           ## all simulations have n=100\n",
    "parameterValues[\"m\"] = [2, 5, 10, 25, 50, 100, 250]\n",
    "parameterValues[\"k\"] = [3, 5, 11, 25]  ## repeat each comb. ten times\n",
    "parameterValues[\"rep\"] = list(range(1, 10+1))\n",
    " ## expand.grid generates data.frame with all combinations of\n",
    " ## supplied arguments\n",
    "simulationParameterGrid = expandGrid(parameterValues)\n",
    " ## we'll say all features are different between group A and B:\n",
    "simulationParameterGrid[\"mEffected\"] = simulationParameterGrid[\"m\"]\n",
    " ## but with an effect size shrinking with mEffected:\n",
    "simulationParameterGrid[\"effectSize\"] = 2.5 /\\\n",
    "                                        np.sqrt(simulationParameterGrid[\"m\"])\n",
    "simulationParameterGrid.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulationParameterGrid.shape[0]  ## len(m) * len(k) * 10 repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our desired simulation parameters nicely organized,\n",
    "we could blast through all of them using a for-loop, but one of\n",
    "the advantages of having our simulation and modeling procedure coded\n",
    "up as a function is that it allows us to adopt a slightly more elegant\n",
    "approach using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## simulate and model one system / data set per row of\n",
    " ## simulationParameterGrid; use list comprehension to do this:\n",
    "modelingResults = [\n",
    "     ## simulationParameterGrid.iloc[i]\n",
    "     ## has all of the arguments for simulateAndKnnModel,\n",
    "     ## but they are packed into single pd.Series;\n",
    "     ## **dict(...) enables function call to unpack pd.Series\n",
    "     ## into separate arguments using index as arg name:\n",
    "    simulateAndKnnModel(**dict(simulationParameterGrid.iloc[i]))\n",
    "    for i in range(simulationParameterGrid.shape[0])\n",
    "]\n",
    " ## package into pd.DataFrame:\n",
    "modelingResults = pd.concat(modelingResults, axis=1).T\n",
    "modelingResults[\"m\"] = modelingResults[\"m\"].astype(int)\n",
    "modelingResults[\"k\"] = modelingResults[\"k\"].astype(int)\n",
    "modelingResults[\"rep\"] = modelingResults[\"rep\"].astype(int)\n",
    "modelingResults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to absorb large quantities of quantitative information\n",
    "visually, so let's repackage and plot these results using `pd.stack`\n",
    "and `plotnine.ggplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggdata = modelingResults.set_index([\"m\", \"k\", \"rep\"])\\\n",
    "                        .stack()\\\n",
    "                        .reset_index()\n",
    "ggdata.columns = [\"m\", \"k\", \"rep\", \"method\", \"estimated accuracy\"]\n",
    "ggdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = ggplot(ggdata, aes(x=\"m\", y=\"estimated accuracy\", color=\"method\"))\n",
    "gg += facet_wrap(\"~k\")\n",
    "gg += stat_smooth()\n",
    "gg += geom_point(alpha=0.6)\n",
    "gg += scale_x_log10()\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure illustrates the degree to which the more flexible (low\n",
    "$k$) knn models overfit relative to the less flexible (high $k$)\n",
    "models: the resubstitution accuracy estimate curves lie considerably\n",
    "above the test accuracy estimate curves for `k=3`, with the\n",
    "difference between the two curves shrinking considerably as $k$ rises\n",
    "upwards towards `k=25`.\n",
    "\n",
    "Cross-Validation \n",
    "================\n",
    "\n",
    "When data are scarce, we'd like to be able to both\n",
    "1.  build a classifier using a large fraction---close to 100% if\n",
    "    possible---of the available sampling units, while\n",
    "2.  assessing classifier performance without suffering\n",
    "    resubstitution bias.\n",
    "\n",
    "We know how to handle 2: split the data into training and test sets,\n",
    "using only training set to build the classifier and only test set for\n",
    "evaluation of performance. Unfortunately this isn't so great for 1!\n",
    "\n",
    "One thing we could do with our data split, however, is to swap which\n",
    "set is used to train and which is used to test. This doesn't\n",
    "immediately address point 1 above, but it does at least allow us to\n",
    "use all of our data to test performance.\n",
    "\n",
    "But we can do better! Why not split our data into three subsets A, B,\n",
    "and C: we can train on (A+B) and test on C, then train on (A+C) and\n",
    "test on B, and finally train on (B+C) and test on A. Now we're making\n",
    "some progress on point 1 above as well as point 2: our training sets\n",
    "are $\\frac{2}{3}$ of our full data set and we end up using 100% of\n",
    "the data for testing!\n",
    "\n",
    "This is the key idea of *cross-validation*\n",
    "([@stone1974cross]), which takes it further to allow for 4-fold,\n",
    "5-fold, 6-fold, \\..., $n$-fold splits of our data set in addition to\n",
    "the 3-fold split just described. The general idea is to fit a model on\n",
    "the data from all but one of the subsets and test on the one held-out\n",
    "subset, repeating this process so that every subset is held-out once.\n",
    "\n",
    "Performance is generally estimated using this procedure by\n",
    "-   computing accuracy (or whatever other metric one might prefer)\n",
    "    separately on each held-out data subset\n",
    "    -   *using the model fit to the the data from all other\n",
    "        subsets* so that\n",
    "    -   in no case is a sampling unit $i$ tested using a fit model for\n",
    "        which $i$ was part of the training set,\n",
    "    \n",
    "-   and then averaging the accuracy estimates from each fold together.\n",
    "\n",
    "We could code this up from scratch, but it's easier (and less\n",
    "bug-prone) to use `cross_val_score` provided by the `sklearn`\n",
    "module `sklearn.model_selection`. Note that unless you want your\n",
    "cross-validation folds to be split based on the order of the data\n",
    "specified by your `DataFrame`s, you will have to shuffle the data\n",
    "yourself prior to cross-validation with `cross_val_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simData = simulate2group(n=100, m=10, mEffected=1, effectSize=2.5)\n",
    " ## recall simData is dictionary with pd.DataFrame simData[\"x\"]\n",
    " ## and class pd.Series simData[\"y\"]\n",
    "cvFolds = 5\n",
    " ## sklearn wants you to shuffle data set prior to splitting into CV folds!\n",
    "shuffle = np.random.permutation(len(simData[\"y\"]))\n",
    "shuffle[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## use shuffle to shuffle both simData[\"x\"] and simData[\"y\"]:\n",
    "simShuffled = {\"x\" : simData[\"x\"].iloc[shuffle],\n",
    "               \"y\" : simData[\"y\"].iloc[shuffle]}\n",
    " ## set up sklearn object to handle cv folds:\n",
    "import sklearn.model_selection as ms\n",
    "cvScheduler = ms.KFold(n_splits = cvFolds)\n",
    " ## now use sklearn.model_selection.cross_val_score to do cross-validation:\n",
    "cvAccs = ms.cross_val_score(\n",
    "    estimator = nbr.KNeighborsClassifier(n_neighbors = 5),\n",
    "    X = simShuffled[\"x\"],   ## shuffled!\n",
    "    y = simShuffled[\"y\"],   ## shuffled by same shuffle!\n",
    "    cv = cvScheduler.split(simShuffled[\"x\"])\n",
    ")\n",
    " ## compute average of accuracy estimates from all 5 CV folds:\n",
    "np.mean(cvAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to repeat the same many-different-simulations excercise\n",
    "I did above comparing resubstitution and test set accuracy estimates,\n",
    "only replace resubstitution with cross-validation using\n",
    "`cross_val_score`. First I'll set up a function to facilitate this repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateAndCrossValidateKnnModel(n, m, k, mEffected, effectSize, rep,\n",
    "                                     cvFolds, **kargs):\n",
    "     ## in case numpy int types have been passed in as arguments:\n",
    "    n=int(n); m=int(m); k=int(k); mEffected=int(mEffected); rep=int(rep)\n",
    "    trainSet = simulate2group(n, m, mEffected=mEffected, effectSize=effectSize)\n",
    "    testSet = simulate2group(n, m, mEffected=mEffected, effectSize=effectSize)\n",
    "    shuffle = np.random.permutation(len(trainSet[\"y\"]))\n",
    "    trainShuffled = trainSet[\"x\"].iloc[shuffle]\n",
    "    trainYShuffled = trainSet[\"y\"].iloc[shuffle]\n",
    "    cvScheduler = ms.KFold(n_splits=int(cvFolds))\n",
    "    knnCvAccs = ms.cross_val_score(\n",
    "        estimator = nbr.KNeighborsClassifier(n_neighbors=k),\n",
    "        X = trainShuffled,\n",
    "        y = trainYShuffled,\n",
    "        cv = cvScheduler.split(trainShuffled)\n",
    "    )\n",
    "    cvAccuracyEst = np.mean(knnCvAccs)\n",
    "    knnFit = nbr.KNeighborsClassifier(n_neighbors=k)\\\n",
    "                .fit(trainSet[\"x\"], trainSet[\"y\"])\n",
    "    testPredictions = knnFit.predict(testSet[\"x\"])\n",
    "    test2by2 = pd.crosstab(testPredictions, testSet[\"y\"])\n",
    "    testAccuracyEst = np.sum(np.diag(test2by2)) / np.sum(np.sum(test2by2))\n",
    "     ## return vector of results along with simulation parameters:\n",
    "    return(pd.Series({\"m\" : m,\n",
    "                      \"k\" : k,\n",
    "                      \"rep\" : rep,\n",
    "                      \"cv\" : cvAccuracyEst,\n",
    "                      \"test\" : testAccuracyEst}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on to list comprehending this function along the rows of\n",
    "`simulationParameterGrid` (we'll re-use the same one from before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## simulate and model one system / data set per row of simulationParameterGrid\n",
    " ## use list comprehension to do this\n",
    "simulationParameterGrid[\"cvFolds\"] = 5\n",
    "cvModelingResults = [\n",
    "    simulateAndCrossValidateKnnModel(**dict(simulationParameterGrid.iloc[i]))\n",
    "    for i in range(simulationParameterGrid.shape[0])\n",
    "]\n",
    " ## package into pd.DataFrame:\n",
    "cvModelingResults = pd.concat(cvModelingResults, axis=1).T\n",
    "cvModelingResults[\"m\"] = cvModelingResults[\"m\"].astype(int)\n",
    "cvModelingResults[\"k\"] = cvModelingResults[\"k\"].astype(int)\n",
    "cvModelingResults[\"rep\"] = cvModelingResults[\"rep\"].astype(int)\n",
    "cvModelingResults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pretty much the same plotting code from before with only\n",
    "the slightest of modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggdata = cvModelingResults.set_index([\"m\", \"k\", \"rep\"])\\\n",
    "                          .stack()\\\n",
    "                          .reset_index()\n",
    "ggdata.columns = [\"m\", \"k\", \"rep\", \"method\", \"estimated accuracy\"]\n",
    "gg = ggplot(ggdata, aes(x=\"m\", y=\"estimated accuracy\", color=\"method\"))\n",
    "gg += facet_wrap(\"~k\")\n",
    "gg += stat_smooth()\n",
    "gg += geom_point(alpha=0.6)\n",
    "gg += scale_x_log10()\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation works! The cross-validated accuracies are pretty much\n",
    "in line with the test accuracies.\n",
    "\n",
    "There is actually a slight *downward* bias in the accuracy\n",
    "estimates produced by cross-validation resulting from the fact that\n",
    "our training sets using 5-fold cross-validation are only 80% the size\n",
    "of the full data set available for training when use the independent\n",
    "test set. Lest you think that this suggests we should always use the\n",
    "largest possible number of cross-validation (CV) folds---that is,\n",
    "$n$---you should know that while increasing the number of CV folds\n",
    "decreases the negative bias in accuracy estimation, it also increases\n",
    "the imprecision (variance) in accuracy estimation. As a rule of thumb,\n",
    "you might consider 5- or 10-fold CV as good default `cvFolds` values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
