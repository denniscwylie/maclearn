{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-Means Clustering \n",
    "=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine\n",
    "from plotnine import ggplot, aes, geom_point, scale_color_manual, xlab, ylab\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    " ## get rid of default gray background:\n",
    "plotnine.theme_set(plotnine.theme_bw())\n",
    " ## load neves data:\n",
    "from load_neves import nevesExpr, nevesAnnot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first example of a simple unsupervised ML algorithm, let's\n",
    "consider $k$-means clustering ([@macqueen1967some]):\n",
    "1.  Initialize $k$ \"centroids\" $\\mathbf{c}_a$\n",
    "    -   bold font indicates a vector;\n",
    "    -   subscript $a$ denotes which cluster and ranges from 1 to $k$.\n",
    "    \n",
    "2.  Assign sampling unit with feature vector $\\mathbf{x}_i$ to nearest cluster:\n",
    "    $$\\label{eq:k-means-1}\n",
    "       \\text{clust}(\\mathbf{x}_i) =\n",
    "       \\underset{a}{\\operatorname{arg\\,min}} \\lVert \\mathbf{x}_i - \\mathbf{c}_a \\rVert$$\n",
    "    -   \"arg min\" with $a$ below it looks at the expression to the\n",
    "        right ($\\lVert \\mathbf{x}_i - \\mathbf{c}_a \\rVert$ here) and\n",
    "        returns the value of $a$ which minimizes it,\n",
    "    -   that is, the cluster $a$ such that the centroid\n",
    "        $\\mathbf{c}_a$ is closest to $\\mathbf{x}_i$.\n",
    "    \n",
    "3.  Reset centroids to mean of associated data:\n",
    "    $$\\label{eq:k-means-2}\n",
    "       \\mathbf{c}_a = \\frac{1}{\\lvert S_a \\rvert} \\sum\\limits_{i \\in S_a} \\mathbf{x}_i$$\n",
    "    -   where the set\n",
    "        $$S_a = \\lbrace i \\mid \\text{clust}(\\mathbf{x}_i) = a \\rbrace$$\n",
    "        contains all sampling units $i$ assigned to cluster $a$.\n",
    "    -   $|S_a|$ is defined as the number of elements in set $S_a$.\n",
    "    \n",
    "4.  Repeat steps 2-3 until convergence (i.e. the clusters don't change anymore).\n",
    "\n",
    "(There are many animations of the $k$-means algorithm in action\n",
    "available online---try googling [\"k-means clustering animation\"](https://lmgtfy.com/?q=k-means+clustering+animation)\n",
    "if you're interested in seeing a few of them.)\n",
    "\n",
    "Let's apply the algorithm, as implemented in `clust.KMeans` (a.k.a. `sklearn.cluster.KMeans`)\n",
    "to the Neves data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "import sklearn.cluster as clust\n",
    "kmFit = clust.KMeans(n_clusters=4).fit(nevesExpr.T)\n",
    "kmPreds = kmFit.predict(nevesExpr.T)\n",
    "kmPreds[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values `kmPreds[i]` are the just the cluster\n",
    "$\\text{clust}(\\mathbf{x}_i)$ for each sample $i$ from 1 to $n=12$ in\n",
    "the Neves data set. How do they compare with the Neves sample groupings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## pd.crosstab counts how many combinations there are of the\n",
    " ## discrete values occurring in one or more vectors passed in\n",
    " ## as arguments:\n",
    "pd.crosstab(kmPreds, nevesAnnot[\"group\"],\n",
    "            rownames=[\"cluster\"], colnames=[\"group\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that this simple unsupervised clustering approach finds\n",
    "clusters mostly---though not completely---the same as the sample\n",
    "experimental groupings.\n",
    "\n",
    "The $k$-means algorithm has an interesting statistical interpretation:\n",
    "the solution locally minimizes\n",
    "$$\\label{eq:k-means-interpretation}\n",
    "\\sum\\limits_{a=1}^k \\sum\\limits_{i \\in S_a} \\left(\\mathbf{x}_i - \\mathbf{c}_a\\right)^2$$\n",
    "\n",
    "which may be regarded as a sum of squared errors if you regard\n",
    "centroid $\\mathbf{c}_a$ as the predicted feature vector for all\n",
    "sampling units $i$ assigned to cluster $a$.\n",
    "\n",
    "The link between $k$-means clustering and statistics suggested by Eq\n",
    "[eq:k-means-interpretation](#eq:k-means-interpretation) can be understood in more depth by\n",
    "deriving the method as an asymptotic limiting case of probabilistic\n",
    "mixture-of-Gaussians model ([@ghahramani2004unsupervised]) (where\n",
    "each Gaussian in the mixture has its own centroid vector\n",
    "$\\mathbf{c}_a$ but all share a common spherical covariance matrix\n",
    "$\\sigma^2 \\mathbf{\\underline{I}}$ and $\\sigma$ is vanishingly small).\n",
    "\n",
    "This derivation explains why, despite being fast and intuitive,\n",
    "$k$-means clustering tends to produce (hyper)spherical, equal-sized\n",
    "clusters whether they are appropriate or not. In real data sets this\n",
    "is often at least somewhat problematic!\n",
    "\n",
    "Hierarchical Clustering \n",
    "=======================\n",
    "\n",
    "Probably the most popular unsupervised clustering method in\n",
    "bioinformatics is *agglomerative hierarchical clustering*\n",
    "([@mary2006introduction; @hastie2009elements]). Hierarchical\n",
    "clustering approaches are so named because they seek to generate a\n",
    "hierarchy of clusterings of the data---generally represented as\n",
    "*dendrogram*, a structure to be discussed shortly.\n",
    "\n",
    "A hierarchy of clusterings is a set of clusterings with, at the lowest\n",
    "level, $n$ distinct clusters---so that no two objects are assigned to\n",
    "the same cluster---followed by a clustering with $n-1$ clusters, in\n",
    "which exactly two objects are assigned to the same cluster, and then a\n",
    "clustering with $n-2$ clusters, and so on, until finally the top level\n",
    "has only one cluster to which all $n$ objects are assigned.\n",
    "\n",
    "Each level of the hierarchy also must be consistent with the level\n",
    "below it: this means that the clustering with $m < n$ clusters must be\n",
    "the result of taking the clustering with $m+1$ clusters and merging\n",
    "two of those $m+1$ clusters together into one. This constraint is what\n",
    "makes it possible to represent the hierarchy with a dendrogram; let's\n",
    "consider an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "import scipy.spatial.distance as dist\n",
    " ## use shc.linkage function to perform hierarchical clustering\n",
    "sampleClust = shc.linkage(\n",
    "    dist.pdist(nevesExpr.T),  ## will discuss both of these arguments\n",
    "    method = \"average\"        ## below!\n",
    ")\n",
    " ## generate dengrogram object using shc.dendrogram\n",
    "cutHeight = 118.5\n",
    "dendro = shc.dendrogram(sampleClust,\n",
    "                        labels = nevesExpr.columns,\n",
    "                        leaf_rotation = 90,\n",
    "                        color_threshold = cutHeight)\n",
    "dendro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different clusterings correspond to different vertical levels in\n",
    "this dendrogram. At the very bottom---below all of the lines---each of\n",
    "the samples are assigned to its own cluster. Then, at the level\n",
    "indicated by the black line here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutHeight = 118.5\n",
    "dendro = shc.dendrogram(sampleClust,\n",
    "                        labels = nevesExpr.columns,\n",
    "                        leaf_rotation = 90,\n",
    "                        color_threshold = cutHeight)\n",
    " ## draw black horizontal line at y=118.5\n",
    "plt.axhline(cutHeight, color=\"black\", linestyle=\"dashed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have joined the two samples TBP-2 and TBP-3 together into a single\n",
    "cluster, since the lines connecting these two samples are below the\n",
    "black line, while each of the other 10 samples is still assigned to its\n",
    "own cluster.\n",
    "\n",
    "We can also extract the cluster identities directly in Python without\n",
    "bothering to look at plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## cut tree at height 118.5:\n",
    "cut118p5 = shc.cut_tree(sampleClust, height=118.5)\n",
    "pd.Series(shc.cut_tree(sampleClust, height=118.5)[:, 0],\n",
    "          index = nevesExpr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## output is vector containing sample cluster labels\n",
    " ## note TBP-2 and TBP-3 are both assigned to cluster 8,\n",
    " ## while all other samples get their own cluster id number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we try a different height cutoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutHeight = 155\n",
    "dendro = shc.dendrogram(sampleClust,\n",
    "                        labels = nevesExpr.columns,\n",
    "                        leaf_rotation = 90,\n",
    "                        color_threshold = cutHeight)\n",
    "plt.axhline(cutHeight, color=\"black\", linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find:\n",
    "-   on the far left, cluster **4**, containing TBP-1, TBP-2,\n",
    "    and TBP-3, then\n",
    "-   cluster **5** containing TRF2-2 and TRF2-3, followed by\n",
    "-   cluster **1** containing only mCherry-3,\n",
    "-   cluster **0** containing TRF2-1, mCherry-1, mCherry-2,\n",
    "-   cluster **2** containing only TAF9-1, and, finally,\n",
    "-   on the far right, cluster **3** containing TAF9-2 and TAF9-3.\n",
    "\n",
    "Often when we want a specific clustering, we want to specify the\n",
    "number of clusters instead of trying to figure out what height to cut\n",
    "at; this can be done with `cut_tree` using the `n_clusters` argument\n",
    "instead of the `height` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(shc.cut_tree(sampleClust, n_clusters=6)[:, 0],\n",
    "          index = nevesExpr.columns)    ## generates same 6 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dissimilarity metrics \n",
    "---------------------\n",
    "When we first ran `shc.linkage`, we supplied two arguments; the first of\n",
    "these was `dist.pdist(nevesExpr.T)`. The `nevesExpr.T` part of this\n",
    "simply takes the transpose of `nevesExpr`, but we haven't seen the function\n",
    "`dist.pdist` before, so let's take a look\n",
    "(using `dist.squareform` as well to better view the object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.squareform(dist.pdist(nevesExpr.T, metric=\"euclidean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done here is to compute the *Euclidean distances* of\n",
    "each of the 12 samples from each of the other 11 samples. The\n",
    "Euclidean distance is defined here, as in geometry, as the square root\n",
    "of the sum of the squared differences in each coordinate of a vector;\n",
    "since this is more easily comprehended via math or code than English\n",
    "words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinateDifferenceSample1Sample2 =\\\n",
    "        nevesExpr.iloc[:, 0] - nevesExpr.iloc[:, 1]\n",
    "np.sqrt( np.sum( coordinateDifferenceSample1Sample2**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## results in same value as\n",
    "dist.squareform(dist.pdist(nevesExpr.T, metric=\"euclidean\"))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want these distances here as a way to measure how dissimilar one\n",
    "sample's expression profile is from another (Euclidean distance is not\n",
    "the only dissimilarity metric which can be used with `shc.linkage`;\n",
    "you can consult the help documentation for `dist.pdist` function and\n",
    "its `metric` argument to see what other options are available). The\n",
    "agglomerative hierarchical clustering algorithm implemented by\n",
    "`shc.linkage` uses these quantified dissimarilities between pairs of\n",
    "samples to decide, at each step, which two clusters to join together\n",
    "from the clustering with `m+1` clusters to form the clustering with\n",
    "`m` clusters.\n",
    "\n",
    "This is easiest to do in the first step, where we start with every\n",
    "sample in its own cluster. In this case, we just pick the two samples\n",
    "with the smallest dissimilarity value (in this case, TBP-2 and\n",
    "TBP-3, with a dissimilarity score of 113.34\n",
    "between them) and join them together into a cluster.\n",
    "\n",
    "Agglomeration Linkage Methods {#eq:hclust-linkage}\n",
    "-----------------------------\n",
    "\n",
    "After we've created a cluster containing two separate objects a new\n",
    "problem confronts us: How do we decide in the next round of clustering\n",
    "whether to join together two singleton objects (objects which are in\n",
    "their own cluster not containing any other objects) or instead to join\n",
    "a singleton object into the two-object cluster we created in our first\n",
    "step?\n",
    "\n",
    "We need a way to assign numeric dissimilarities between\n",
    "*clusters* of objects based on the numeric dissimilarities we've\n",
    "already calculated between individual objects. In the example I've\n",
    "constructed here, I've done this using a very simple approach: The\n",
    "dissimilarity between cluster A and cluster B is defined to be the\n",
    "average of the dissimarilities between all pairs of objects we can\n",
    "form taking one object from cluster A and the other object in the pair\n",
    "from cluster B. This is the meaning of the code `method = \"average\"`\n",
    "in the `shc.linkage` call above.\n",
    "\n",
    "This way of defining dissimilarities between clusters based on\n",
    "dissimilarities between objects is known as \"average linkage.\" Many\n",
    "alternatives exist; one particularly common one  is \"complete linkage.\"\n",
    "Complete linkage defines the dissimilarity between cluster A and\n",
    "cluster B as the largest dissimilarity value for all pairs of objects\n",
    "taken one from A and the other from B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleClust2 = shc.linkage(dist.pdist(nevesExpr.T),\n",
    "                           method = \"complete\")\n",
    "dendro = shc.dendrogram(sampleClust2,\n",
    "                        labels = nevesExpr.columns,\n",
    "                        leaf_rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With complete linkage we can see that at the higher levels of the\n",
    "dendrogram we obtain different clusterings than we did with average\n",
    "linkage. In particular, with average linkage the three samples TBP-1,\n",
    "TBP-2, and TBP-3---are the last to be merged together with the\n",
    "remainder of the sample set, while with complete linkage this is not\n",
    "the case.\n",
    "\n",
    "Clustered Heatmaps \n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    " ## usually most interested in expression levels relative to mean:\n",
    "heatData = (nevesExpr.T - nevesExpr.mean(axis=1)).T\n",
    " ## often want to limit dynamic range heatmap considers so that\n",
    " ## color palette range is not dominated by a few extreme values:\n",
    "heatData[heatData > 2] = 2; heatData[heatData < -2] = -2\n",
    " ## pheatmap is not a grammar-of-graphics style plotting function:\n",
    " ## specify all options as arguments to single function call\n",
    " ## intead of building plot up in modular fashion:\n",
    "sns.clustermap(heatData, method=\"average\", cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Clusters from Hierachical Clustering \n",
    "-----------------------------------------------\n",
    "So\\...how do we predict the cluster assignment for a new sampling unit\n",
    "with feature vector $\\mathbf{x}$ at each level of the clustering\n",
    "hierarchy?\n",
    "\n",
    "We don't. As I mentioned in the introduction, hierarchical clustering\n",
    "is one of the few machine learning algorithms which doesn't really fit\n",
    "into the \"algorithms to learn algorithms to predict\" scheme.\n",
    "\n",
    "Principal Component Analysis \n",
    "============================\n",
    "\n",
    "(NOTE: in discussion of PCA, I will reserve symbol $x$ for PCA\n",
    "*scores* as opposed to feature values, to accord with R's use of\n",
    "`pca$x`. I will shift *for this section only* to use of $z$\n",
    "for feature values. I will make one exception at the end in\n",
    "defining the `extractPCs` function, as it will be used again in\n",
    "later sections where I will return to use of $x$ for feature values.)\n",
    "There are many different ways to describe the underyling idea of PCA ([@roweis1999unifying; @izenman2008modern]);\n",
    "here's one: PCA fits a series of *principal components* to model\n",
    "the expression levels $z_{ig}$ of all genes $g$ across all samples\n",
    "$i$. We'll start with a single principal component (PC1) model:\n",
    "$$\\label{eq:single-pc-model}\n",
    "z_{ig} = \\mu_g + x_{i1} r_{g1} + e^{(1)}_{ig}$$\n",
    "\n",
    "where:\n",
    "-   $\\mu_g = \\frac{1}{n} \\sum_i z_{ig}$ is the mean expression level\n",
    "    of gene $g$,\n",
    "-   $x_{i1}$ is the \"score\" of sample $i$ on PC1,\n",
    "-   $r_{g1}$ is the \"loading\" of gene $g$ on PC1, and\n",
    "-   $e^{(1)}_{ig}$ is the error residual for gene $g$ on sample $i$\n",
    "    using PC1 model.\n",
    "\n",
    "Fitting PC1 thus requires estimating $x_{i1}$ for all samples $i$ and\n",
    "$r_{g1}$ for all genes $g$. This is generally done so as to minimize\n",
    "the sum of squared residuals\n",
    "$\\sum_{i,g} \\left( e^{(1)}_{ig} \\right)^2$ (PCA is another\n",
    "least-squares algorithm). It so happens that there is a beautifully\n",
    "elegant and efficient algorithm solving just this problem via\n",
    "something known in linear algebra as singular value decomposition\n",
    "(SVD) implemented in the function `extractPCs` included in the file\n",
    "`maclearn_utils_2020.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## will use extractPCs function from maclearn_utils_2020.py for PCA\n",
    " ## on transposed version of nevesExpr, nevesExpr.T,\n",
    " ## since extractPCs assumes variables (genes) are in columns, not rows\n",
    " ## * z_ig = nevesExpr[g, i] = nevesExpr.T.iloc[i, g]\n",
    "from maclearn_utils_2020 import extractPCs\n",
    "pca = extractPCs(nevesExpr.T)\n",
    "dir(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## the PCA scores x_i1 for samples are found in pca.x:\n",
    "pca.x[0:5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## the PCA loadings r_g1 for genes are found in pca$rotation:\n",
    "pca.rotation[0:5, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that I extracted only the first column from both\n",
    "`pca.x` and `pca.rotation`. This is because these matrices\n",
    "contain PCs beyond PC1. For instance, the second column of each of\n",
    "these matrices corresponds to PC2, which is defined by\n",
    "$$\\label{eq:two-pc-model}\n",
    "z_{ig} = \\mu_g + x_{i1} r_{g1} + x_{i2} r_{g2} + e^{(2)}_{ig}$$\n",
    "\n",
    "where PC1 is obtained from the single-PC model and PC2 is then again\n",
    "fit to minimize the remaining $\\sum_{i,g} \\left( e^{(2)}_{ig} \\right)^2$.\n",
    "\n",
    "This process can be repeated to obtain higher order PCs as well; in\n",
    "general, the $k^{\\text{th}}$ PC has\n",
    "-   scores $x_{ik}$ which can be found in the Python object `pca.x[i, k]` and\n",
    "-   loadings $r_{gk}$ stored in `pca.rotation[g, k]`\n",
    "\n",
    "and minimizes the squared sum of the error residuals\n",
    "$$\\label{eq:k-pc-model-residual}\n",
    "e^{(k)}_{ig} = z_{ig} - \\mu_g - \\sum_{j=1}^k x_{ij} r_{gj}$$\n",
    "\n",
    "It is worth noting that after fitting $n$ PCs (recall $n$ is the\n",
    "number of samples, 12 here), there is no error\n",
    "left---that is, $e^{(n)}_{ig}=0$---so that we will have a perfect fit\n",
    "for $z_{ig}$:\n",
    "$$\\begin{aligned}\n",
    "z_{ig} &=  \\mu_g + \\sum_{j=1}^n x_{ij} r_{gj} \\\\\n",
    " &= \\mu_g + \\left[ \\mathbf{\\underline{X}} \\mathbf{\\underline{R}}^{\\text{T}} \\right]_{ig}\n\\end{aligned}$$\n",
    "where in Eq [eq:matrix-product-notation-pca](#eq:matrix-product-notation-pca) we have switched\n",
    "over to matrix notation. This can be confirmed in Python via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaFit = (nevesExpr.mean(axis=1).values + np.dot(pca.x, pca.rotation.T)).T\n",
    "nevesExpr.iloc[0:3, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaFit[0:3, 0:3]  ## same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you may be wondering what we do with all of these sample scores\n",
    "$x_{ik}$ and gene loadings $r_{gk}$. Well, the first thing we usually\n",
    "do is make a PCA plot by plotting $x_{i2}$ (which we usually label as\n",
    "simply \"PC2\") on the vertical axis against $x_{i1}$ (\"PC1\") on the\n",
    "horizontal axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaData = pd.DataFrame(pca.x[:, 0:2],\n",
    "                       columns = [\"PC1\", \"PC2\"],\n",
    "                       index = nevesExpr.columns)\n",
    " ## add in sample annotation info\n",
    "pcaData[\"group\"] = nevesAnnot.loc[pcaData.index, \"group\"]\n",
    " ## and sample names\n",
    "pcaData[\"sample\"] = pcaData.index\n",
    "gg = ggplot(pcaData, aes(x=\"PC1\", y=\"PC2\", color=\"group\", label=\"sample\"))\n",
    "gg += geom_point(size=2.5, alpha=0.75)\n",
    "colVals = [\"goldenrod\", \"orangered\", \"lightseagreen\", \"darkgray\"]\n",
    "gg += scale_color_manual(values=colVals)\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us something interesting: despite PCA knowing nothing about\n",
    "the sample groupings, it has fit PC1 so as to split the TBP\n",
    "experimental group apart from all others (in the sense that the TBP\n",
    "group samples have large positive scores $x_{i1}$ while all other\n",
    "samples have negative PC1 scores). This tells us that, in some sense,\n",
    "TBP is the most different sample group relative to all of the others.\n",
    "\n",
    "Modeling Expression Levels with First PC (or Two) \n",
    "-------------------------------------------------\n",
    "\n",
    "I introduced PCA as a way to model expression levels using\n",
    "multiplicative factors of sample scores and gene loadings. You might\n",
    "well wonder how good it is as such a model, so let's go ahead and look\n",
    "at the expression patterns (sometimes called \"profiles\") of a gene\n",
    "or two.\n",
    "\n",
    "Let's start with the gene TBP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nevesExpr.T\n",
    "geneIndex = np.where(z.columns == \"Tbp\")[0]\n",
    "tbpData = pd.DataFrame({\n",
    "    \"pc1model\" : np.mean(z[\"Tbp\"]) +\\\n",
    "                 pca.x[:, 0] * pca.rotation[geneIndex, 0],\n",
    "    \"actual\" : z[\"Tbp\"],\n",
    "    \"group\" : nevesAnnot[\"group\"]\n",
    "})\n",
    "tbpPlot = ggplot(tbpData, aes(\"pc1model\", \"actual\", color=\"group\"))\n",
    "tbpPlot += geom_point()\n",
    "tbpPlot += scale_color_manual(values=colVals)\n",
    "tbpPlot += xlab(\"PC1 modeled TBP expression\")\n",
    "tbpPlot += ylab(\"Actual TBP expression\")\n",
    "print(tbpPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PC1 (only) model (calculated based on Eq\n",
    "[eq:single-pc-model](#eq:single-pc-model) above) for TBP expression appears to do a\n",
    "pretty job! But perhaps we should be suspicious that this performance\n",
    "may not be totally representative, given that we noted that PC1 split\n",
    "the \"TBP\" sample group out from the other samples. Indeed, recall\n",
    "that this sample group is defined by the presence of an experimental\n",
    "RNAi transgene for TBP, and indeed we see that expression of the TBP\n",
    "gene itself is quite significantly reduced in this sample group\n",
    "relative to all others.\n",
    "\n",
    "So let's consider a different gene, CecA2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneIndex = np.where(z.columns == \"CecA2\")[0]\n",
    "ceca2Data = pd.DataFrame({\n",
    "    \"pc1model\" : np.mean(z[\"CecA2\"]) +\\\n",
    "                 pca.x[:, 0] * pca.rotation[geneIndex, 0],\n",
    "    \"actual\" : z[\"CecA2\"],\n",
    "    \"group\" : nevesAnnot[\"group\"]\n",
    "})\n",
    "ceca2Plot = ggplot(ceca2Data, aes(\"pc1model\", \"actual\", color=\"group\"))\n",
    "ceca2Plot += geom_point()\n",
    "ceca2Plot += scale_color_manual(values=colVals)\n",
    "ceca2Plot += xlab(\"PC1 modeled CECA2 expression\")\n",
    "ceca2Plot += ylab(\"Actual CECA2 expression\")\n",
    "print(ceca2Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So\\...PC1 model does not do so well this time! It can't, because\n",
    "`pca.x[:, 0]` assigns the most extreme values to samples from the\n",
    "TBP group, while the actual expression levels of CecA2 in the TBP\n",
    "sample group are quite middle-of-the-road. But don't despair, we can\n",
    "always try PC1+PC2 model as defined by Eq [eq:two-pc-model](#eq:two-pc-model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceca2Data[\"pc1and2model\"] = np.mean(z[\"CecA2\"]) +\\\n",
    "                            pca.x[:, 0] * pca.rotation[geneIndex, 0] +\\\n",
    "                            pca.x[:, 1] * pca.rotation[geneIndex, 1]\n",
    "ceca2Plot = ggplot(ceca2Data, aes(\"pc1and2model\", \"actual\", color=\"group\"))\n",
    "ceca2Plot += geom_point()\n",
    "ceca2Plot += scale_color_manual(values=colVals)\n",
    "ceca2Plot += xlab(\"PC1+PC2 modeled CECA2 expression\")\n",
    "ceca2Plot += ylab(\"Actual CECA2 expression\")\n",
    "print(ceca2Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe not perfect, but definitely much better! In case you're curious\n",
    "as to why I picked CecA2 in particular, it is the gene with the\n",
    "largest positive loading on PC2 (as you can confirm by running\n",
    "`np.argmax(pca.rotation[:, 1])` if you're so inclined). Thus we\n",
    "might have expected it to be a gene for which the PC1+PC2 model would\n",
    "be notably better than a PC1-only model.\n",
    "\n",
    "Percent Variation Explained by PC $k$ \n",
    "---------------------------------------------------\n",
    "\n",
    "This leads us to a generally ill-understood point in PCA:\n",
    "quantification of how much each PC contributes to modeling the gene\n",
    "expression values. Perhaps you recall the use of $R^2$ to characterize\n",
    "the quality of fit for linear models (regression, ANOVA, etc.) in\n",
    "statistics; we can use this idea in PCA as well, but we have to decide\n",
    "what to do about the fact that it is a truly *multivariate*\n",
    "model: We are simultaneously modeling thousands of genes!\n",
    "\n",
    "The general approach taken is more or less the simplest generalization\n",
    "of the $R^2$ idea available. Recalling that $R^2$ gives the \"percent\n",
    "of variation explained,\" quantified as the percentage reduction in\n",
    "sum of squared residuals, we first define\n",
    "$$\\begin{aligned}\n",
    "\\sigma^2_0 &= \\frac{1}{n-1} \\sum_{i, g} \\left( z_{ig} - \\mu_g \\right)^2 \\\\\n",
    "\\sigma^2_k &= \\frac{1}{n-1} \\sum_{i, g} \\left( e^{(k)}_{ig} \\right)^2\n\\end{aligned}$$\n",
    "\n",
    "where the $k^{\\text{th}}$ matrix of error residuals $e^{(k)}_{ig}$ is\n",
    "defined by Eq [eq:k-pc-model-residual](#eq:k-pc-model-residual). We can regard Eq\n",
    "[eq:pca-sigmaSq0](#eq:pca-sigmaSq0) as just Eq [eq:pca-sigmak](#eq:pca-sigmak) with $k=0$ if\n",
    "we accept the natural definition\n",
    "$$e^{(0)}_{ig} = z_{ig} - \\mu_g$$\n",
    "for the error residuals of a \"no-PC model.\" Then we can also rewrite\n",
    "Eq [eq:k-pc-model-residual](#eq:k-pc-model-residual) for any $k > 0$ as as:\n",
    "$$e^{(k)}_{ig} = e^{(k-1)}_{ig} - x_{ik} r_{gk}$$\n",
    "\n",
    "Let's take a second to do a few of these calculations in R, using\n",
    "the name `resid0[i, g]` to represent $e^{(0)}_{ig}$, `resid1[i, g]` for\n",
    "$e^{(1)}_{ig}$, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## matrix of residuals e^(0)_ig:\n",
    "resid0 = nevesExpr.T - np.mean(nevesExpr, axis=1)  \n",
    " ## now use outer function to construct matrix with i, g entry\n",
    " ##  pca.x[i, 0] * pca.rotation[g, 0]\n",
    " ## and subtract this from resid0 to obtain resid1:\n",
    "resid1 = resid0 - np.outer(pca.x[:, 0], pca.rotation[:, 0])\n",
    " ## resid1 contains error residuals e^(1)_ig after fitting PC1-only model\n",
    "resid2 = resid1 - np.outer(pca.x[:, 1], pca.rotation[:, 1])\n",
    " ## resid2 contains error residuals e^(2)_ig from PC1+PC2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, using `sigmaSq0` to represent $\\sigma^2_0$, `sigmaSq1` for\n",
    "$\\sigma^2_1$, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nevesExpr.shape[1]  ## number of samples (12)\n",
    "sigmaSq0 = np.sum(np.sum(resid0**2)) / (n-1)\n",
    "sigmaSq1 = np.sum(np.sum(resid1**2)) / (n-1)\n",
    "sigmaSq2 = np.sum(np.sum(resid2**2)) / (n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the variance explained by PC $k$ is defined to be the overall\n",
    "reduction in variance associated with adding PC $k$ into our model,\n",
    "and is given by\n",
    "$$\\label{eq:pc-k-r2}\n",
    "\\Delta \\sigma^2_k = \\sigma^2_{k-1} - \\sigma^2_k$$\n",
    "and the \"fraction of variation\" explained by PC $k$ is finally\n",
    "$$\\label{eq:pct-var-pc-k}\n",
    "\\frac{\\Delta \\sigma^2_k}{\\sigma^2_0} =\n",
    "\\frac{\\Delta \\sigma^2_k}{\\sum_{k=1}^n{\\Delta \\sigma^2_k}}$$\n",
    "where the right-hand side of Eq [eq:pct-var-pc-k](#eq:pct-var-pc-k) holds because,\n",
    "as we verified above, the variance $\\sigma^2_n$ remaining after\n",
    "fitting all $n$ PCs is 0; that is\n",
    "$$\\begin{aligned}\n",
    "0 &= \\sigma^2_n \\\\\n",
    "  &= \\sigma^2_0 - \\left( \\sigma^2_0 - \\sigma^2_1 \\right) - \\left( \\sigma^2_1 - \\sigma^2_2 \\right) - \\cdots - \\left( \\sigma^2_{n-1} - \\sigma^2_n \\right) \\\\\n",
    "  &= \\sigma^2_0 - \\sum_{k=1}^n \\left( \\sigma^2_{k-1} - \\sigma^2_k \\right) \\\\\n",
    "  &= \\sigma^2_0 - \\sum_{k=1}^n{\\Delta \\sigma^2_k} \\\\\n",
    "\\sigma^2_0 &= \\sum_{k=1}^n{\\Delta \\sigma^2_k}\n\\end{aligned}$$\n",
    "The output `pca` from `extractPCs` stores the quantities\n",
    "$\\sqrt{\\Delta \\sigma^2_k}$ in the field `pca.sdev`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## let's compare:\n",
    "np.sqrt(sigmaSq0 - sigmaSq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.sdev[0]  ## same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## what about the second PC?\n",
    "np.sqrt(sigmaSq1 - sigmaSq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.sdev[1]  ## same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## fraction of variation explained by PC1, \"from scratch\":\n",
    "(sigmaSq0 - sigmaSq1) / sigmaSq0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## fraction of variation explained by PC1, using pca$dev:\n",
    "pca.sdev[0]**2 / np.sum(pca.sdev**2)  ## same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## fraction of variation explained by PC2, from scratch:\n",
    "(sigmaSq1 - sigmaSq2) / sigmaSq0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## fraction of variation explained by PC2, using pca$dev:\n",
    "pca.sdev[1]**2 / np.sum(pca.sdev**2)  ## same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the first PC in the `pca` model explains\n",
    "34.6% of the overall\n",
    "gene expression variance, while the second PC explains an additional\n",
    "20.2% of overall gene\n",
    "expression variance. Let's update the axis labels of our PCA plot to\n",
    "include this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctVar = 100 * pca.sdev**2 / np.sum(pca.sdev**2)\n",
    "gg += xlab(\"PC1 (\" + str(np.round(pctVar[0], 1)) + \"% explained var.)\")\n",
    "gg += ylab(\"PC2 (\" + str(np.round(pctVar[1], 1)) + \"% explained var.)\")\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that these percentages reflect the fraction of variance\n",
    "explained across *all* genes; some genes (e.g., TBP) may be much\n",
    "better explained by PC1 alone than are other genes (e.g., CecA2)!\n",
    "\n",
    "Extracting Sampling Unit Scores {#eq:pca-predictions}\n",
    "-------------------------------\n",
    "\n",
    "Here is the definition of the function `extractPCs` for learning\n",
    "the PCs from a training set which returns an object which, in addition to\n",
    "containing the PCA information we've been using above, also works as\n",
    "a function `extractor` for assessing the sampling unit scores for a\n",
    "test set `newdata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## arguments to extractPCs\n",
    " ## - mat is matrix or data.frame of feature values\n",
    " ## - m is number of principal component features to extract;\n",
    " ##   if None, returns the full PCA decomposition\n",
    "def extractPCs(mat, m=None, *args):\n",
    "     ## assume mat is samples-in-rows, genes-in-columns format!\n",
    "    if m is None:\n",
    "        m = np.min(mat.shape)    \n",
    "    mu = colMeans(x)  ## training-set-estimated mean expression of each gene:\n",
    "     ## use singular value decomposition (SVD) to compute PCs:\n",
    "    svdOut = np.linalg.svd(mat - mu, full_matrices=False)\n",
    "    x = svdOut[0] * svdOut[1]  ## same as R's prcomp out$x\n",
    "    rotation = svdOut[2].T     ## same as R's prcomp out$rotation\n",
    "    sdev = svdOut[1] / np.sqrt(len(svdOut[1])-1)  ## same as R's prcomp out$sdev\n",
    "    extractor = lambda newdata : np.dot(newdata-mu, rotation[:, 0:m])\n",
    "    extractor.sdev = sdev\n",
    "    extractor.rotation = rotation    \n",
    "    extractor.center = mu\n",
    "    extractor.x = x\n",
    "    extractor.m = m\n",
    "     ## return the function \"extractor\" which can be applied to newdata;\n",
    "     ## this function yields coordinates of samples in newdata in PC-space\n",
    "     ## learned from the training data passed in as x argument.\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the math implemented by `extractPCs`, you'll find that it\n",
    "assigns to a sampling unit with feature vector $\\mathbf{z}$ a PC-$k$ score of:\n",
    "$$\\label{eq:pca-predict-score}\n",
    "\\sum_g \\left(z_g - \\mu_g\\right) r_{gk}$$\n",
    "which is by definition the $k^{\\text{th}}$ component of the vector\n",
    "$\\mathbf{\\underline{R}}^{\\text{T}}(\\mathbf{z}-\\boldsymbol{\\mu})$.\n",
    "\n",
    "Note that there are two distinct ways in which PCA is, in a technical\n",
    "sense, an \"algorithm which learns to predict\":\n",
    "1.  It can be used to predict gene expression values (via the\n",
    "    restriction of the sum in Eq [eq:n-pc-model-residual](#eq:n-pc-model-residual) to the\n",
    "    desired number of PCs), and\n",
    "2.  it can be used to \"predict\" (we would more commonly say\n",
    "    *extract*) the location of a new sampling unit in the PC space\n",
    "    defined by the training set via Eq [eq:pca-predict-score](#eq:pca-predict-score).\n",
    "I'll note that while both of these modes of prediction differ somewhat\n",
    "from the ordinary language meaning of prediction, the second does make\n",
    "PCA a very useful *feature extraction* method, as we will see a\n",
    "bit later!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
