{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Hess Data Set \n",
    "=============\n",
    "\n",
    "The Hess data set I initially mentioned in the introductory notes\n",
    "consists of microarray data taken from fine-needle biopsies taken from\n",
    "breast cancer patients. A number of patient characteristics were\n",
    "collected, but the main focus of the modeling that we will be\n",
    "doing---like the modeling that Hess et al. were doing!\n",
    "([@hess2006pharmacogenomic])---will be the sensivity to\n",
    "preoperative chemotherapy, with the patients divided into those who\n",
    "exhibited residual disease (RD) or those who did not and were thus\n",
    "classified as have pathologic complete response (pCR) to chemotherapy.\n",
    "In order to load the Hess data in, let's re-define the function `rt`\n",
    "we used to load the Neves data in before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## define convenience function for loading tabular data\n",
    " ## (just using read.table with different default options)\n",
    "rt = function(f) {\n",
    "    read.table(f, sep=\"\\t\", row.names=1, header=TRUE,\n",
    "               check.names=FALSE, comment.char=\"\", quote=\"\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Hess et al. obtained two separate data sets, a training set which they\n",
    "used to develop a classifier for RD-vs-pCR, and a test set which they\n",
    "used to assess the performance of the resulting classifier. Let's load\n",
    "in the training data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## training set:\n",
    "hessTrain = rt(\"data/HessTrainingData.tsv.gz\")\n",
    "hessTrainAnnot = rt(\"data/HessTrainingAnnotation.tsv\")\n",
    " ## align annotation data.frame with expression data:\n",
    "hessTrainAnnot = hessTrainAnnot[colnames(hessTrain), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "And now the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## test set:\n",
    "hessTest = rt(\"data/HessTestData.tsv.gz\")\n",
    "hessTestAnnot = rt(\"data/HessTestAnnotation.tsv\")\n",
    " ## align annotation data.frame with expression data:\n",
    "hessTestAnnot = hessTestAnnot[colnames(hessTest), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Taking a quick look at the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "hessTrain[1:5, 1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "we see that the rows here are not annotated by gene ids but instead by\n",
    "*probe set* ids. We'll load in the microarray probe annotations\n",
    "mapping these probe sets back to genes as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "probeAnnot = rt(\"data/U133A.tsv.gz\")\n",
    " ## align hessTrain and hessTest to probeAnnot:\n",
    "hessTrain = hessTrain[rownames(probeAnnot), ]\n",
    "hessTest = hessTest[rownames(probeAnnot), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "For ease of classification, we'd like to extract the class labels from\n",
    "columns of the sample annotation files into `factor` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "hessTrainY = factor(hessTrainAnnot$pCRtxt)\n",
    "names(hessTrainY) = rownames(hessTrainAnnot)\n",
    "hessTestY = factor(hessTestAnnot$pCRtxt)\n",
    "names(hessTestY) = rownames(hessTestAnnot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Finally, let's take a quick look at the test and training set data put\n",
    "together via a PCA plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## use cbind to column-bind the training and test data sets together:\n",
    "combinedData = cbind(hessTrain, hessTest)\n",
    "pca = prcomp(t(combinedData))\n",
    "library(ggplot2)\n",
    "theme_set(theme_bw())  ## get rid of gray backgrounds in ggplot\n",
    "ggdata = data.frame(\n",
    "    PC1 = pca$x[ , 1],\n",
    "    PC2 = pca$x[ , 2],\n",
    "    set = c(rep(\"train\", ncol(hessTrain)), rep(\"test\", ncol(hessTest)))\n",
    ")\n",
    "gg = ggplot(ggdata, aes(x=PC1, y=PC2, color=set))\n",
    "gg = gg + geom_point(size=2)\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "This shows an uncomfortable fact about real-world applications of\n",
    "machine learning: when training and test sets are collected\n",
    "separately, there tend to be systematic differences between them. This\n",
    "can result in degraded test-set performance even when models have been\n",
    "carefully constructed using the most effective algorithms available!\n\n",
    "$k$-Nearest Neighbors (knn) \n",
    "=========================================\n",
    "\n",
    "The $k$-neighest neighbors, or knn, algorithm\n",
    "([@cover1968estimation]) is a particularly simple and democratic\n",
    "approach to classification:\n",
    "To classify sampling unit $i$ with feature values $x_ig$:\n",
    "-   find the $k$ sampling units $\\{j_1, j_2, \\ldots, j_k\\}$ from the\n",
    "    training set *most similar* to $i$: these are the \"nearest\n",
    "    neighbors\"\n",
    "-   calculate the fraction $\\frac{\\sum_b y_{j_b}}{k}$ of the nearest\n",
    "    neighbors which have $y_{j_{b}} = 1$: this is the knn model-predicted\n",
    "\tprobability that $y_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## extract vector of gene expression values for test sample 1:\n",
    "featuresForTestSample1 = hessTest[ , 1]\n",
    " ## calculate distance of each training sample from test sample 1:\n",
    " ## note: subtraction of vector from matrix is column-wise in R!\n",
    "euclideanDistancesFromTrainSamples = sqrt(\n",
    "    colSums( (hessTrain - featuresForTestSample1)^2 )\n",
    ")\n",
    " ## what are the 9 nearest neighbors\n",
    " ## and their distances from test sample 1?\n",
    "nn = sort(euclideanDistancesFromTrainSamples)[1:9]\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## what are their classifications?\n",
    "hessTrainY[names(nn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## 9-nn model predicted probability of RD for test sample 1 is:\n",
    "sum( hessTrainY[names(nn)] == \"RD\" ) / 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Here I should hasten to point out that all of the ML algorithms we\n",
    "will study here have been been packaged up into more efficient and\n",
    "user-friendly R routines, so there is really no need to go through the\n",
    "pain of re-implementing them from scratch! (I just wanted to give you\n",
    "a sense of how simple knn in particular is \"under the hood.\")\n",
    "\n",
    "Here is the way I would actually suggest to apply the knn algorithm in\n",
    "R (using the `knn3` function from the library `caret`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "## install.packages(\"caret\")  ## uncomment and run if necessary\n",
    "library(caret)\n",
    " ## fit model object obtained by running:\n",
    "knnFit = knn3(x = t(hessTrain),   ## knn3 wants features-in-columns\n",
    "              y = hessTrainY,     ## recall hessTrainY is factor!\n",
    "              k = 9)\n",
    " ## can then generate test set predictions using knnFit:\n",
    "knnTestPredictionProbs = predict(knnFit, t(hessTest))\n",
    " ## good to inspect results of predict method in R\n",
    " ## (output from predict not standardized from one alg to next):\n",
    "head(knnTestPredictionProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## what is predicted probability RD for test sample 1 again?\n",
    "knnTestPredictionProbs[1, \"RD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "For some R modeling functions---including `knn3`---can use `type`\n",
    "argument to specify what format you want predictions in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "knnTestPredictionClass = predict(knnFit, t(hessTest), type=\"class\")\n",
    "head(knnTestPredictionClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## use table function to generate 2x2 contingency table:\n",
    "contingency = table(knnTestPredictionClass, hessTestY)\n",
    "contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "The 2x2 contingency table is a very useful and commonly presented\n",
    "summary of binary classification results. If one class is regarded as\n",
    "\"positive\" and one as \"negative,\" the various cells of the 2x2\n",
    "table can be labeled:\n",
    "\n",
    "| .               | Actual (-)             | Actual (+)            |\n",
    "|-----------------|------------------------|-----------------------|\n",
    "| Predicted (-)   | True Negatives (TN)    | False Negatives (FN)  |\n",
    "| Predicted (+)   | False Positives (FP)   | True Positives (TP)   |\n",
    "\n",
    "Notice that:\n",
    "-   the diagonal elements of the contingency table correspond to\n",
    "    accurate classifications, and that\n",
    "-   every (classifiable) sampling unit will fall into one of the\n",
    "    four cells.\n",
    "\n",
    "Thus we can calculate the fraction of sampling units classified\n",
    "correctly---referred to in ML contexts as the *accuracy* of the\n",
    "model fit---by dividing the sum of the diagonals of the contingency\n",
    "table by the sum of all four entries in the contingency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "estimatedAccuracy = sum(diag(contingency)) / sum(contingency)\n",
    "estimatedAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Overfitting \n",
    "===========\n",
    "\n",
    "Let's use the data for two specific microarray probes,\n",
    "205548_s_at and 201976_s_at to fit a knn model with $k=27$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## we'll go ahead and transpose the data.frame to have\n",
    " ## features-in-columns for convenience:\n",
    "twoProbeData = t(hessTrain)[ , c(\"205548_s_at\", \"201976_s_at\")]\n",
    " ## let's use friendlier gene names instead of probe ids here:\n",
    "colnames(twoProbeData) =\n",
    "        probeAnnot[colnames(twoProbeData), \"Gene.Symbol\"]\n",
    "twoProbeFitK27 = knn3(twoProbeData, hessTrainY, k=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "I'm using two probes because I want to be able to make a contour plot\n",
    "of the *decision boundary* of the knn classifier. I'm going to\n",
    "use a function saved in the file `ggfuntile.R` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "source(\"ggfuntile.R\")  ## defines predictionContour function\n",
    "predictionContour(twoProbeFitK27, twoProbeData, hessTrainY, \"k = 27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Looking at this decision boundary you might think this classifier\n",
    "looks too conservative about calling pCR: Surely we could push that\n",
    "boundary to the left a bit to catch a few more of those open\n",
    "downward-pointing triangles? Thinking about this a bit more, it seems\n",
    "that perhaps our choice of parameter $k=27$ is a bit high; after all,\n",
    "27 is almost a third of all 82 samples in the\n",
    "Hess training set. The appropriate neighborhood for points in the\n",
    "upper right hand corner of the contour plot may be better estimated\n",
    "with a more local knn model defined by, say, $k=9$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "twoProbeFitK9 = knn3(twoProbeData, hessTrainY, k=9)\n",
    "predictionContour(twoProbeFitK9, twoProbeData, hessTrainY, \"k = 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "That does look somewhat better! Many more pCR samples correctly called\n",
    "at the cost of only one extra misclassified RD sample. But perhaps we\n",
    "could do better still with an even more local model---let's try $k=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "twoProbeFitK3 = knn3(twoProbeData, hessTrainY, k=3)\n",
    "predictionContour(twoProbeFitK3, twoProbeData, hessTrainY, \"k = 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Hmmm\\...this does appear to catch a few more pCR samples from the\n",
    "training set, but we seem to have generated some swiss cheese-like\n",
    "holes in the RD predicted region, along with a very convoluted bay\n",
    "and peninsula in the center right portion of the main decision\n",
    "boundary. Still, this seems like a very subjective complaint---let's\n",
    "look at some accuracy estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## define pair of convenience functions to minimize repeated code:\n",
    "contingencize = function(knnFit, data, y) {\n",
    "    table(predict(knnFit, data, type=\"class\"), y)\n",
    "}\n",
    "estimateAccuracyFrom2x2 = function(twoByTwo) {\n",
    "    sum(diag(twoByTwo)) / sum(twoByTwo)\n",
    "}\n",
    "twoByTwo27 = contingencize(twoProbeFitK27, twoProbeData, hessTrainY)\n",
    "estimateAccuracyFrom2x2(twoByTwo27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "twoByTwo9 = contingencize(twoProbeFitK9, twoProbeData, hessTrainY)\n",
    "estimateAccuracyFrom2x2(twoByTwo9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "twoByTwo3 = contingencize(twoProbeFitK3, twoProbeData, hessTrainY)\n",
    "estimateAccuracyFrom2x2(twoByTwo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "So does this really mean the swiss-cheese decision region is the\n",
    "best\\...?\n",
    "\n",
    "Of course not! All of the accuracy estimates we just made suffer from\n",
    "what's called *resubstitution bias* because we tested the model\n",
    "on the same data set that was used to train it. Let's clean that up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## extract test data for our two favorite probes\\...\n",
    "twoProbeTest = t(hessTest)[ , c(\"205548_s_at\", \"201976_s_at\")]\n",
    "colnames(twoProbeTest) =\n",
    "        probeAnnot[colnames(twoProbeTest), \"Gene.Symbol\"]\n",
    " ## now let's take another stab at accuracy estimations:\n",
    "twoByTwo27 = contingencize(twoProbeFitK27, twoProbeTest, hessTestY)\n",
    "estimateAccuracyFrom2x2(twoByTwo27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "twoByTwo9 = contingencize(twoProbeFitK9, twoProbeTest, hessTestY)\n",
    "estimateAccuracyFrom2x2(twoByTwo9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "twoByTwo3 = contingencize(twoProbeFitK3, twoProbeTest, hessTestY)\n",
    "estimateAccuracyFrom2x2(twoByTwo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "While we may be a bit disappointed to see that the best accuracy\n",
    "estimate from the test set is worse than the worst accuracy estimate\n",
    "from resubstitution of the training set, we can find solace in noting\n",
    "that the $k=3$ model with it's bizarre decision boundary is no longer\n",
    "judged the best.\n",
    "\n",
    "This is the classic problem of *overfitting*: Models with more\n",
    "freedom to fit very complex patterns in the training data set---such\n",
    "as our very local low-$k$ knn model---have a tendency to find\n",
    "\"signals\" which are not reproducible in independent data sets, even\n",
    "those of a very similar nature.\n",
    "\n",
    "Here's an example where you can see the overfitting coming without any\n",
    "computation at all: What do you think the resubstitution accuracy of a\n",
    "1-nearest neighbor model would be? As a hint, you might think about what\n",
    "the nearest neighbor of training sample $i$ is in the training set\\...\n\n",
    "knn Simulation \n",
    "==============\n",
    "\n",
    "At this point I will digress away from analysis of the Hess microarray\n",
    "data for a bit to consider simulated data sets. Simulated data can be\n",
    "useful because:\n",
    "1.  we know the true model used to generate the data exactly, and\n",
    "2.  we can systematically vary any parameters that appear in the\n",
    "    data generation model so as to study how well our ML algorithms work\n",
    "    in a range of situations.\n",
    "Let's define a function `simulate2group` for simulating\n",
    "-   a simple data set with `n` sampling units (or simulated samples)\n",
    "    and `m` features (simulated genes, if you like),\n",
    "-   with the sampling units divided into two groups A and B,\n",
    "-   and `mEffected` $\\leq$ `m` of the features being shifted by\n",
    "    `effectSize` units on average in group B relative to group A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "simulate2group = function(n = 100,   ## number simulated samples\n",
    "                          m = 1000,  ## number simulated genes\n",
    "                          nA = ceiling(0.5*n),  ## first nA samples = group A\n",
    "                                                ## last (n-nA) samples = group B\n",
    "                          mEffected = 10,   ## first mEffected genes will have\n",
    "                                            ## different expression in group B\n",
    "                          effectSize = 1 ## how many expression units difference\n",
    "                                         ## between groups for mEffected genes\n",
    "                          ) {\n",
    "    x = matrix(rnorm(n*m), nrow=n, ncol=m)  ## simulate iid expression values\n",
    "                                            ## (highly unrealistic, but easy)\n",
    "    y = factor(c(rep(\"A\", nA), rep(\"B\", (n-nA))))\n",
    "    colnames(x) = paste0(\"g\", 1:m)   ## gene labels like g1, g2, etc.\n",
    "    rownames(x) = paste0(\"i\", 1:n)   ## sample labels like i1, i2, etc.\n",
    "    names(y) = rownames(x)   ## assign sample labels as names of grouping vector\n",
    "    x[y==\"B\", 1:mEffected] = x[y==\"B\", 1:mEffected] + effectSize\n",
    "    return(list(x=data.frame(x), y=y))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Because of the second advantage associated with simulated data\n",
    "above---the ability to repeat the analysis while varying simulation\n",
    "parameters---I'm going to package our data generation, model fitting,\n",
    "and model assessment procedure up into a function of those simulation\n",
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "simulateAndKnnModel = function(n, m, k, mEffected, effectSize,\n",
    "                               rep=1, ...) {\n",
    "    require(caret)\n",
    "    trainSet = simulate2group(n = n,\n",
    "                              m = m,\n",
    "                              mEffected = mEffected,\n",
    "                              effectSize = effectSize)\n",
    "    testSet = simulate2group(n = n,\n",
    "                             m = m,\n",
    "                             mEffected = mEffected,\n",
    "                             effectSize = effectSize)\n",
    "    knnFit = knn3(trainSet$x, trainSet$y, k)\n",
    "    resubstitutionPredictions = predict(knnFit, trainSet$x, type=\"class\")\n",
    "     ## construct contingency table and use to estimate accuracy:\n",
    "    resub2by2 = table(resubstitutionPredictions, trainSet$y)\n",
    "    resubAccuracyEst = sum(diag(resub2by2)) / sum(resub2by2)\n",
    "     ## do same thing for testPredictions:\n",
    "    testPredictions = predict(knnFit, testSet$x, type=\"class\")\n",
    "    test2by2 = table(testPredictions, testSet$y)\n",
    "    testAccuracyEst = sum(diag(test2by2)) / sum(test2by2)\n",
    "     ## return vector of results along with simulation parameters:\n",
    "    return(c(m = m,\n",
    "             k = k,\n",
    "             rep = rep,  ## rep included to track repetition index\n",
    "             resubstitution = resubAccuracyEst,\n",
    "             test = testAccuracyEst))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Here's an example using this function to assess the performance of a\n",
    "5-nearest neighbors model (`k=5`) on a simulated data set of\n",
    "`n=100` sampling units with `m=10` features, of which\n",
    "`mEffected=1` feature has values elevated by `effectSize=2.5`\n",
    "units in group B relative to group A (we'll rely on the\n",
    "`simulate2group` default value of `nA=ceiling(0.5*n)=50` to\n",
    "specify that half of the sampling units are in group A and the other\n",
    "half in group B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "simulateAndKnnModel(n=100, m=10, k=5, mEffected=1, effectSize=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "The function reports out some of the simulation parameters along with\n",
    "the estimated accuracy results so that we can keep track of what\n",
    "parameters went into eah simulation when we repeat this procedure many\n",
    "times. We're going to do this by setting up a `data.frame` with one\n",
    "row per simulation and columns specifying the parameters to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## expand.grid generates data.frame with all combinations of\n",
    " ## supplied arguments\n",
    "simulationParameterGrid = expand.grid(\n",
    "    n = 100,                ## all simulations have n=100\n",
    "    m = c(2, 5, 10, 25, 50, 100, 250),\n",
    "    k = c(3, 5, 11, 25),\n",
    "    rep = 1:10              ## repeat each combination ten times\n",
    ")\n",
    " ## we'll say all features are different between group A and B:\n",
    "simulationParameterGrid$mEffected = simulationParameterGrid$m\n",
    " ## but with an effect size shrinking with mEffected:\n",
    "simulationParameterGrid$effectSize =\n",
    "        2.5 / sqrt(simulationParameterGrid$mEffected)\n",
    "head(simulationParameterGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "nrow(simulationParameterGrid)  ## length(m) * length(k) * 10 repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Now that we have our desired simulation parameters nicely organized,\n",
    "we could blast through all of them using a `for`-loop, but one of\n",
    "the advantages of having our simulation and modeling procedure coded\n",
    "up as a function is that it allows us to adopt a slightly more elegant\n",
    "approach using the `apply` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## simulate and model one data set per row of simulationParameterGrid\n",
    " ## use base-R apply function to do this\n",
    "modelingResults = apply(\n",
    "    X = simulationParameterGrid,\n",
    "    MARGIN = 1,  ## iterate over rows (first margin) of X argument\n",
    "    FUN = function(dfrow) {\n",
    "         ## dfrow has all of the arguments for simulateAndKnnModel,\n",
    "         ## but they are packed into single vector;\n",
    "         ## do.call enables function call to unpack a (named) list\n",
    "         ## into separate (named) arguments:\n",
    "        do.call(simulateAndKnnModel, args=as.list(dfrow))\n",
    "    }\n",
    ")\n",
    "dim(modelingResults)  ## apply here produces matrix with 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "                      ## per iteration, so let's transpose:\n",
    "modelingResults = t(modelingResults)\n",
    "head(modelingResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "It's easier to absorb large quantities of quantitative information\n",
    "visually, so let's repackage and plot these results using `tidyr`\n",
    "and `ggplot2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "## install.packages(\"tidyr\")  ## uncomment and run if necessary\n",
    "library(tidyr)\n",
    "ggdata = data.frame(modelingResults) %>%\n",
    "    pivot_longer(resubstitution:test,\n",
    "                 names_to = \"method\",\n",
    "                 values_to = \"estimated accuracy\")\n",
    "ggdata$k = factor(\n",
    "    paste0(\"k=\", ggdata$k),\n",
    "    levels = unique(paste0(\"k=\", ggdata$k))\n",
    ")\n",
    "head(ggdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "gg = ggplot(ggdata, aes(x=m, y=`estimated accuracy`, color=method))\n",
    "gg = gg + facet_wrap(~k)\n",
    "gg = gg + stat_smooth(method.args=list(degree=1))\n",
    "gg = gg + geom_point(alpha=0.6)\n",
    "gg = gg + scale_x_log10()\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "This figure illustrates the degree to which the more flexible (low\n",
    "$k$) knn models overfit relative to the less flexible (high $k$)\n",
    "models: the resubstitution accuracy estimate curves lie considerably\n",
    "above the test accuracy estimate curves for `k=3`, with the\n",
    "difference between the two curves shrinking considerably as $k$ rises\n",
    "upwards towards `k=25`.\n\n",
    "Cross-Validation \n",
    "================\n",
    "\n",
    "When data are scarce, we'd like to be able to both\n",
    "1.  build a classifier using a large fraction---close to 100% if\n",
    "    possible---of the available sampling units, while\n",
    "2.  assessing classifier performance without suffering\n",
    "    resubstitution bias.\n",
    "\n",
    "We know how to handle 2: split the data into training and test sets,\n",
    "using only training set to build the classifier and only test set for\n",
    "evaluation of performance. Unfortunately this isn't so great for 1!\n",
    "\n",
    "One thing we could do with our data split, however, is to swap which\n",
    "set is used to train and which is used to test. This doesn't\n",
    "immediately address point 1 above, but it does at least allow us to\n",
    "use all of our data to test performance.\n",
    "\n",
    "But we can do better! Why not split our data into three subsets A, B,\n",
    "and C: we can train on (A+B) and test on C, then train on (A+C) and\n",
    "test on B, and finally train on (B+C) and test on A. Now we're making\n",
    "some progress on point 1 above as well as point 2: our training sets\n",
    "are $\\frac{2}{3}$ of our full data set and we end up using 100% of\n",
    "the data for testing!\n",
    "\n",
    "This is the key idea of *cross-validation*\n",
    "([@stone1974cross]), which takes it further to allow for 4-fold,\n",
    "5-fold, 6-fold, \\..., $n$-fold splits of our data set in addition to\n",
    "the 3-fold split just described. The general idea is to fit a model on\n",
    "the data from all but one of the subsets and test on the one held-out\n",
    "subset, repeating this process so that every subset is held-out once.\n",
    "\n",
    "Performance is generally estimated using this procedure by\n",
    "-   computing accuracy (or whatever other metric one might prefer)\n",
    "    separately on each held-out data subset\n",
    "    -   *using the model fit to the the data from all other\n",
    "        subsets* so that\n",
    "    -   in no case is a sampling unit $i$ tested using a fit model for\n",
    "        which $i$ was part of the training set,\n",
    "    \n",
    "-   and then averaging the accuracy estimates from each fold together.\n",
    "\n",
    "We could code this up from scratch, but it's easier (and less\n",
    "bug-prone) to use the `train` function provided by the `caret`\n",
    "library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "simData = simulate2group(n=100, m=10, mEffected=1, effectSize=2.5)\n",
    " ## recall simData is named list with data.frame simData$x\n",
    " ## and grouping factor simData$y\n",
    "cvFolds = 5\n",
    "caretOut = train(simData$x, simData$y, method=\"knn\",\n",
    "                 trControl=trainControl(method=\"cv\", number=cvFolds))\n",
    "caretOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Notice that all we told `train` about our modeling strategy (ML\n",
    "algorithm) was the string `\"knn\"`; this works because `train`\n",
    "happens to know about knn. Since we didn't tell it anything about\n",
    "details like what $k$ value to use, `train` went ahead and picked\n",
    "its own values to try, selecting the one that produced the best\n",
    "cross-validation-estimated accuracy value.\n",
    "While it was convenient to just tell `train` to use\n",
    "`method=\"knn\"`, we can get a lot more control over exactly what\n",
    "algorithms are `train`ed by putting together a named `list` of the\n",
    "components `train` needs in a `method` argument like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "knnCaretized = list(\n",
    "     ## knn3 is in caret library, don't need any others:\n",
    "    library = NULL,\n",
    "     ## caret works with both Classification and Regression,\n",
    "     ## need to tell it we want to do Classification:\n",
    "    type = \"Classification\",\n",
    "     ## tell caret what parameters exist in this model:\n",
    "    parameters = data.frame(parameter=\"k\", class=\"integer\", label=\"n_nbrs\"),\n",
    "     ## and also what value(s) of those parameters to try:\n",
    "    grid = function(x, y, len=NULL, ...) {data.frame(k=9)},\n",
    "     ## provide caret a function to generate a fit model:\n",
    "     ## (args x, y, param, and \\...; all parameters go in list param):\n",
    "    fit = function(x, y, param, ...) {knn3(x, y, param$k)},\n",
    "     ## also provide a function to predict classifications using fit\n",
    "     ## (important that argument names be modelFit, newdata, and ...):\n",
    "    predict = function(modelFit, newdata, ...) {\n",
    "        predict(modelFit, newdata, type=\"class\")\n",
    "    },\n",
    "     ## and finally a function to make probabilistic predictions:\n",
    "    prob = function(modelFit, newdata, ...) {predict(modelFit, newdata)}\n",
    ")\n",
    "caretOut = train(simData$x, simData$y, method=knnCaretized,\n",
    "                 trControl=trainControl(method=\"cv\", number=cvFolds))\n",
    "caretOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "While this is a lot more work on our end, I'm going to use this way of\n",
    "providing `method` arguments to `train` from now on, both because\n",
    "for some of the more complicated modeling strategies we'll consider\n",
    "it's the only way to get `train` to run them and because `train`\n",
    "tends to run faster when you tell it exactly what to do. While\n",
    "`caret` offers many useful features, it has a deserved reputation\n",
    "for tying up your computational resources for a while!\n",
    "\n",
    "Now I'm going to repeat the same many-different-simulations excercise\n",
    "I did above comparing resubstitution and test set accuracy estimates,\n",
    "only replace resubstitution with cross-validation using\n",
    "`train`. First I'll set up a function to facilitate this repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "simulateAndCrossValidateKnnModel =\n",
    "        function(n, m, k, mEffected, effectSize, rep, cvFolds, ...) {\n",
    "    require(caret)\n",
    "    trainSet = simulate2group(n=n, m=m,\n",
    "                              mEffected=mEffected, effectSize=effectSize)\n",
    "    testSet = simulate2group(n=n, m=m,\n",
    "                             mEffected=mEffected, effectSize=effectSize)\n",
    "    knnCaretized = list(\n",
    "        library = NULL,\n",
    "        type = \"Classification\",\n",
    "        parameters = data.frame(parameter=\"k\", class=\"integer\", label=\"n_nbrs\"),\n",
    "        grid = function(x, y, len=NULL, ...) {data.frame(k=k)},\n",
    "        fit = function(x, y, param, ...) {knn3(x, y, param$k)},\n",
    "        predict = function(modelFit, newdata, ...) {\n",
    "            predict(modelFit, newdata, type=\"class\")\n",
    "        },\n",
    "        prob = function(modelFit, newdata, ...) {predict(modelFit, newdata)}\n",
    "    )\n",
    "    caretOut = train(x = trainSet$x,\n",
    "                     y = trainSet$y,\n",
    "                     method = knnCaretized,\n",
    "                     trControl = trainControl(method=\"cv\", number=cvFolds))\n",
    "    cvAccuracyEst = caretOut$results$Accuracy\n",
    "    testPredictions = predict(caretOut$finalModel, testSet$x, type=\"class\")\n",
    "    test2by2 = table(testPredictions, testSet$y)\n",
    "    testAccuracyEst = sum(diag(test2by2)) / sum(test2by2)\n",
    "    return(c(m = m,\n",
    "             k = k,\n",
    "             rep = rep,\n",
    "             cv = cvAccuracyEst,\n",
    "             test = testAccuracyEst))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Now, on to `apply`ing this function to the\n",
    "`simulationParameterGrid` (we'll re-use the same one from before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    " ## add a column to simulationParameterGrid with number of cvFolds:\n",
    "simulationParameterGrid$cvFolds = 5\n",
    " ## and now we're ready to go!\n",
    "cvModelingResults = t(apply(\n",
    "    X = simulationParameterGrid,\n",
    "    MARGIN = 1,\n",
    "    FUN = function(dfrow) {\n",
    "        do.call(simulateAndCrossValidateKnnModel, args=as.list(dfrow))\n",
    "    }\n",
    "))\n",
    "head(cvModelingResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "We can use pretty much the same plotting code from before with only\n",
    "the slightest of modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "outputs": [],
   "source": [
    "ggdata = data.frame(cvModelingResults) %>%\n",
    "    pivot_longer(cv:test,\n",
    "                 names_to = \"method\",\n",
    "                 values_to = \"estimated accuracy\")\n",
    "ggdata$k = factor(\n",
    "    paste0(\"k=\", ggdata$k),\n",
    "    levels = unique(paste0(\"k=\", ggdata$k))\n",
    ")\n",
    "gg = ggplot(ggdata, aes(x=m, y=`estimated accuracy`, color=method))\n",
    "gg = gg + facet_wrap(~k)\n",
    "gg = gg + stat_smooth(method.args=list(degree=1))\n",
    "gg = gg + geom_point(alpha=0.6)\n",
    "gg = gg + scale_x_log10()\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
   "source": [
    "Cross-validation works! The cross-validated accuracies are pretty much in line with the test accuracies.\n",
    "\n",
    "There is actually a slight *downward* bias in the accuracy\n",
    "estimates produced by cross-validation resulting from the fact that\n",
    "our training sets using 5-fold cross-validation are only 80% the size\n",
    "of the full data set available for training when use the independent\n",
    "test set. Lest you think that this suggests we should always use the\n",
    "largest possible number of cross-validation (CV) folds---that is,\n",
    "$n$---you should know that while increasing the number of CV folds\n",
    "decreases the negative bias in accuracy estimation, it also increases\n",
    "the imprecision (variance) in accuracy estimation. As a rule of thumb,\n",
    "you might consider 5- or 10-fold CV as good default `cvFolds` values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
