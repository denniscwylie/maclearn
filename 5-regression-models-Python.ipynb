{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Models \n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine\n",
    "from plotnine import qplot, stat_smooth\n",
    "plotnine.theme_set(plotnine.theme_bw())\n",
    "import sklearn as sk\n",
    "\n",
    "from load_hess import hessTrain, hessTrainAnnot, hessTrainY, hessTest, hessTestAnnot, hessTestY, probeAnnot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our study of classification modeling on pause for a moment\n",
    "and briefly consider regression instead. As a reminder, these two\n",
    "terms are generally distinguished in supervised ML contexts by the\n",
    "nature of the output to be predicted:\n",
	"\n",
    "**Classification**\n",
    ":   models predict discrete class labels, while\n",
	"\n",
    "**Regression**\n",
    ":   models predict numeric values.\n",
    "\n",
    "There are certainly weird edge cases that blur these boundaries, but\n",
    "we won't get into any of those here!\n",
    "\n",
    "I'm going to jump right into an example using the Hess data set here:\n",
    "Modeling the numeric field `DLDA30.Value` from `hessTrainAnnot`\n",
    "using the gene expression values from `hessTrain`. More\n",
    "specifically, using 10 probe sets selected on the basis of correlation\n",
    "with the desired output to, which will be facilitated by defining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absPearsonCorrelation(x, y):\n",
    "     ## assume x is samples-in-rows, genes-in-columns format!\n",
    "    dx = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "    dy = (y - y.mean()) / y.std()\n",
    "    r = np.dot(dy, dx) / (len(y)-1.0)\n",
    "    return np.abs(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our feature-selected linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.pipeline as pl\n",
    "fsLinPipeline = pl.Pipeline([\n",
    "    (\"featsel\", fs.SelectKBest(absPearsonCorrelation, k=10)),\n",
    "    (\"regressor\", lm.LinearRegression())\n",
    "])\n",
    "fsLinFit = deepcopy(fsLinPipeline).fit(hessTrain.T,\n",
    "                                       hessTrainAnnot[\"DLDA30.Value\"])\n",
    "fsLinTrainPreds = fsLinFit.predict(hessTrain.T)\n",
    " ## estimate R^2 (perhaps should write R**2 for Python):\n",
    "from scipy import stats\n",
    "stats.pearsonr(fsLinTrainPreds, hessTrainAnnot[\"DLDA30.Value\"])[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplot(pd.Series(fsLinTrainPreds, index=hessTrain.columns),\n",
    "      hessTrainAnnot[\"DLDA30.Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! Of course, this is comparing predictions to\n",
    "resubsitution-based predictions, so that may or may not be\n",
    "meaningful. Let's try looking at the test set predictions instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsLinTestPreds = fsLinFit.predict(hessTest.T)\n",
    "stats.pearsonr(fsLinTestPreds, hessTestAnnot[\"DLDA30.Value\"])[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplot(pd.Series(fsLinTestPreds, index=hessTest.columns),\n",
    "      hessTestAnnot[\"DLDA30.Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation can be useful with regression just as it is in\n",
    "classification, and can be performed using `cross_val_score` in a\n",
    "similar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\n",
    "np.random.seed(321)\n",
    "shuffle = np.random.permutation(hessTrain.shape[1])\n",
    "trainShuffled = hessTrain.T.iloc[shuffle]\n",
    "trainYShuffled = hessTrainAnnot[\"DLDA30.Value\"][shuffle]\n",
    "cvScheduler = ms.KFold(n_splits=5)\n",
    "fsLinCvR2 = ms.cross_val_score(estimator = fsLinPipeline,\n",
    "                               X = trainShuffled,\n",
    "                               y = trainYShuffled,\n",
    "                               cv = cvScheduler.split(trainShuffled),\n",
    "                               scoring = \"explained_variance\")\n",
    "np.mean(fsLinCvR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lest you get too excited about these results, I should disclose that\n",
    "`DLDA30.Value` is itself the output from a linear classification\n",
    "algorithm applied by Hess et al.\n",
    "\n",
    "Regressing Noise \n",
    "----------------\n",
    "\n",
    "Having seen what regression results look like from an ML standpoint\n",
    "when everything goes smoothly and there's a nice consistent and easily\n",
    "found signal shared by both training and test data sets, let's\n",
    "consider the opposite extreme of no real signal at all. To that end,\n",
    "we'll define a vector of output `noise` unrelated to any input\n",
    "feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "noise = np.random.randn(hessTrain.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onto fitting the noise; we'll re-use `fsLinPipeline` from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsNoiseFit = deepcopy(fsLinPipeline).fit(hessTrain.T, noise)\n",
    "fsLinNoisePreds = fsNoiseFit.predict(hessTrain.T)\n",
    "stats.pearsonr(fsLinNoisePreds, noise)[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplot(pd.Series(fsLinNoisePreds, index=hessTrain.columns),\n",
    "      pd.Series(noise, index=hessTrain.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That resubstitution-estimated $R^2$ value is clearly overfit!\n",
    "Cross-validation to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = np.random.permutation(hessTrain.shape[1])\n",
    "trainShuffled = hessTrain.T.iloc[shuffle]\n",
    "noiseShuffled = noise[shuffle]\n",
    "cvScheduler = ms.KFold(n_splits=5)\n",
    "fsNoiseCvR2 = ms.cross_val_score(\n",
    "    estimator = fsLinPipeline,\n",
    "    X = trainShuffled,\n",
    "    y = noiseShuffled,\n",
    "    cv = cvScheduler.split(trainShuffled),\n",
    "    scoring = \"explained_variance\"\n",
    ")\n",
    "np.mean(fsNoiseCvR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've been (correctly) keeping the feature selection step\n",
    "\"under cross-validation,\" meaning that we re-select a (potentially\n",
    "different!) feature set in each fold of cross-validation making sure\n",
    "to exclude the held-out samples from the calculation of feature scores\n",
    "(Pearson correlations here). This is very important---feature\n",
    "selection is a supervised ML step and can be very sensitive to\n",
    "overfitting!\n",
    "\n",
    "In order to demonstrate this, let's see what happens if we incorrectly\n",
    "apply feature selection prior to cross-validation of only the\n",
    "regression fitting step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## how much overfitting results from feature selection alone?\n",
    "probeAbsCors = absPearsonCorrelation(hessTrain.T, noise)\n",
    "probeAbsCors = pd.Series(probeAbsCors, index=hessTrain.index)\n",
    "topFeatsWholeTrain = hessTrain.T.loc[\n",
    "    :, probeAbsCors.sort_values(ascending=False).index[0:10]\n",
    "]\n",
    "topFeatsWholeTrainShuffled = topFeatsWholeTrain.iloc[shuffle]\n",
    "badLinCvR2 = ms.cross_val_score(\n",
    "    estimator = lm.LinearRegression(),\n",
    "    X = topFeatsWholeTrainShuffled,\n",
    "    y = noiseShuffled,\n",
    "    cv = cvScheduler.split(topFeatsWholeTrainShuffled),\n",
    "    scoring = \"explained_variance\"\n",
    ")\n",
    "np.mean(badLinCvR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, apparent $R^2$ much higher using\n",
    "feature-independent noise using \"cross-validation\" but forgetting to\n",
    "take the overfitting resulting from feature selection into account\n",
    "when compared to the estimate obtained when feature selection is  done\n",
    "correctly! Always, always, *always* keep feature selection under\n",
    "cross-validation.\n",
    "\n",
    "Regularization \n",
    "==============\n",
    "\n",
    "Let's go back and look at a couple of the probe sets chosen among the\n",
    "ten features used to predict `DLDA30.Value` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoProbeSets = pd.Series([\"203928_x_at\", \"203929_s_at\"])\n",
    "twoProbeSets.isin(hessTrain.index[fsLinFit[0].get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplot(hessTrain.T.loc[:, twoProbeSets[0]],\n",
    "      hessTrain.T.loc[:, twoProbeSets[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting---the measured expression values of these two probe\n",
    "sets are quite similar! There is in fact a good explanation for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probeAnnot.loc[twoProbeSets.values, \"Gene.Symbol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microarrays often have multiple probe sets targeting the same gene,\n",
    "and, as a result, in many---though not all!---cases, these probe sets\n",
    "will pick up very similar signals. If one such probe set is correlated\n",
    "with the desired output to be predicted, the other will thus also tend\n",
    "to exhibit such correlation; this is what is happening here.\n",
    "\n",
    "Given this build up, one might expect that the linear model fit using\n",
    "these features (along with 8 others in this case) would assign similar\n",
    "coefficients to both. Does it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsLinCoef = pd.Series(fsLinFit[1].coef_,\n",
    "                      index = hessTrain.index[fsLinFit[0].get_support()])\n",
    "fsLinCoef.loc[twoProbeSets.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so much---not only are the magnitudes of the coefficients very\n",
    "different, but they even have opposite signs!\n",
    "\n",
    "Lest you think that perhaps the linear model has found some useful\n",
    "difference between these two probe sets that isn't immediately\n",
    "apparent to us in the Hess data set, consider the following simulated\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "x = np.random.randn(10, 2)\n",
    "x[:, 1] = x[:, 0] + 0.01 * x[:, 1]   ## force approximate collinearity\n",
    "y = -x[:, 0] - x[:, 1] + np.random.randn(10) ## true beta_1 = beta_2 = -1\n",
    "lm.LinearRegression().fit(x, y).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in fact a well-known artifact of linear models often referred\n",
    "to as the problem of multicollinearity. One way of dealing with it is\n",
    "to manually remove variables one at a time until you've gotten rid of\n",
    "it, but since this is a course in machine learning we will not do\n",
    "that.\n",
    "\n",
    "An alternate solution is to modify the algorithm for fitting a linear\n",
    "model by incorporating something called *regularization*.\n",
    "\n",
    "Unregularized (standard) linear regression uses maximum likelihood to\n",
    "fit the coefficients $\\beta_g$, where $g$ indexes features $g$, by\n",
    "ordinary least-squares (OLS) estimator:\n",
    "$$\\label{eq:ols}\n",
    "\\hat{\\beta}_0, \\hat{\\boldsymbol{\\beta}} =\n",
    "\\underset{\\beta_0, \\boldsymbol{\\beta}}{\n",
    "  \\operatorname{arg\\,min}} \\,\n",
    "    \\sum\\limits_i \\left(\n",
    "       y_i - \\beta_0 - \\boldsymbol{\\beta} \\cdot \\mathbf{x}_i\n",
    "    \\right)^2$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is the vector of feature values $x_{ig}$ for\n",
    "sampling unit $i$, is the vector of coefficients $\\beta_g$, and\n",
    "$\\boldsymbol{\\beta} \\cdot \\mathbf{x}_i = \\sum_g \\beta_g x_{ig}$ is the\n",
    "\"dot product\" of the two vectors.\n",
    "\n",
    "Since $\\hat{y}_i = \\beta_0 + \\boldsymbol{\\beta} \\cdot \\mathbf{x}_i$ is\n",
    "the formula applied by linear regression to predict the value $y_i$\n",
    "for sampling unit $i$, Eq [eq:ols](#eq:ols) says we want to choose the\n",
    "coefficients $\\beta_g$ to minimize the sum of squared\n",
    "*error residuals* $y_i - \\hat{y}_i$.\n",
    "\n",
    "Regularization modifies Eq [eq:ols](#eq:ols) by adding a penalty term:\n",
    "$$\\label{eq:ols-penalized}\n",
    "\\hat{\\beta}_0, \\hat{\\boldsymbol{\\beta}} =\n",
    "\\underset{\\beta_0, \\boldsymbol{\\beta}}{\n",
    "  \\operatorname{arg\\,min}} \\, \\left\\{ \\,\n",
    "    \\sum\\limits_i \\left(\n",
    "       y_i - \\beta_0 - \\boldsymbol{\\beta} \\cdot \\mathbf{x}_i\n",
    "    \\right)^2 + \\phi \\sum\\limits_g \\left| \\beta_g \\right|^p \\right\\}$$\n",
    "\n",
    "where the exponent $p=1$ for L1, or \"lasso,\" regression\n",
    "([@tibshirani1996regression]), or $p=2$ for L2, or \"ridge,\"\n",
    "regression ([@tikhonov1943stability; @hoerl1962application]). This\n",
    "has the effect of biasing the choice of coefficients $\\beta_g$ towards\n",
    "0 by an amount dependent on the strength of the $\\phi$ of the\n",
    "regulaization applied.\n",
    "\n",
    "(If you're partial to Bayesian statistics, you might find it\n",
    "interesting to note that L1 regression can be derived from assuming a\n",
    "Laplace-distributed prior for the coefficients $\\beta_g$, while L2\n",
    "regression can similarly be derived assuming a more pedestrian\n",
    "Gaussian-distributed prior for the $\\beta_g$ [@park2008bayesian].)\n",
    "Let's try L2 regularization out using `sklearn.linear_model.Ridge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## alpha argument to lm.Ridge is, confusingly, regularization\n",
    " ## strength parameter (akin to lamda in R's glmnet)\n",
    "l2mod = lm.Ridge(alpha=0.5).fit(x, y)\n",
    "l2mod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better! What if we try L1 regularization using\n",
    "`sklearn.linear_model.Lasso`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1mod = lm.Lasso(alpha=0.05).fit(x, y)\n",
    "l1mod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, L1 doesn't look so great, but before you write it off,\n",
    "let me give you a bit of background. L2 regularization is older, much\n",
    "easier (and faster) to fit, and tends to \"split the difference\"\n",
    "between collinear predictors---as it did here---while L1\n",
    "regularization is newer, trickier (and slower) to fit, while tending\n",
    "to pick a few variables to assign high magnitude coefficients to while\n",
    "giving all others either exactly 0 or very low magnitudes. That is,\n",
    "L1/lasso regularization is essentially an *embedded feature selection*\n",
    "algorithm!\n",
    "\n",
    "The multicollinearity problem becomes increasingly severe as the\n",
    "dimensionality of the data set increases until it breaks the classical\n",
    "linear modeling framework entirely when the number of features exceeds\n",
    "the number of sampling units in the training set. Regularization fixes\n",
    "this and allows fitting such \"overparametrized\" linear models.\n",
    "\n",
    "Logistic Regression \n",
    "===================\n",
    "\n",
    "Linear models can be used for classification as well as\n",
    "regression. The most popular linear model for classification goes\n",
    "under the confusing name \"logistic regression,\" despite the fact\n",
    "that it is indeed a classication algorithm.\n",
    "\n",
    "The idea of logistic regression is to build a linear model to predict\n",
    "the \"logit-transformed\" probability that a sampling unit should be\n",
    "given a classification label $y=1$ (as opposed to the other possible\n",
    "label $y=0$), where the logit function is\n",
    "$$\\label{eq:logit-function}\n",
    "\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$$\n",
    "The logit function stretches the range of probabilities (from 0 to 1)\n",
    "out to range all the way from $-\\infty$ to $+\\infty$: this is good\n",
    "because it turns out to be difficult to fit linear models well in such\n",
    "a way as to constrict the output range to a narrow interval like 0 to\n",
    "1.\n",
    "\n",
    "It turns out that if the coefficients $\\beta_g$ are a linear model for\n",
    "$\\text{logit}(p)$, then the predicted probability of the\n",
    "classification label $y$ taking the value 1 (i.e. whichever class has\n",
    "been declared \"positive\") for a sampling unit with feature values\n",
    "$x_g$ wrapped up into vector $\\mathbf{x}$ is\n",
    "$$\\label{eq:logistic}\n",
    "\\hat{p} = \\text{expit}(\\beta_0 + \\boldsymbol{\\beta} \\cdot \\mathbf{x})$$\n",
    "where\n",
    "$$\\label{eq:expit}\n",
    "\\text{expit}(u) = \\frac{1}{1+\\text{exp}(-u)}$$\n",
    "is the *logistic*, or inverse-logit, function. Eq\n",
    "[eq:logistic](#eq:logistic) holds because expit is indeed the functional\n",
    "inverse of the logit function:\n",
    "$\\text{expit}\\!\\left(\\text{logit}(p)\\right) = p$ for all\n",
    "$p \\in (0, 1)$ .\n",
    "\n",
    "Logistic regression is a type of *generalized linear model*, or\n",
    "GLM ([@nelder1972generalized; @agresti2015foundations]).\n",
    "\n",
    "Logistic regression suffers from the same sort of multicollinearity\n",
    "problems as linear regression and hence requires one (or more) of\n",
    "feature selection, feature extraction, and/or regularization for\n",
    "application in high-dimensional (more features than sampling units)\n",
    "contexts. Here we'll connect our $t$-test feature selector upstream of\n",
    "a `logisticFitter` in a simple ML pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsLogisticPipeline = pl.Pipeline([\n",
    "    (\"featsel\", fs.SelectKBest(fs.f_regression, k=10)),\n",
    "    (\"classifier\", lm.LogisticRegression(penalty=\"l2\", C=1e6, max_iter=1000))\n",
    "])\n",
    " ## the C=1e6 argument above required to get (almost) unregularized\n",
    " ## logistic regression using sklearn\n",
    "fsLogisticFit = deepcopy(fsLogisticPipeline).fit(hessTrain.T, hessTrainY)\n",
    "fsLogisticTestPredictionProbs = fsLogisticFit.predict_proba(hessTest.T)\n",
    "fsLogisticTestPredictionProbs[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsLogisticTestPredictionClass = fsLogisticFit.predict(hessTest.T)\n",
    "pd.crosstab(fsLogisticTestPredictionClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the features selected by the pipeline here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticFeats = hessTrain.index[fsLogisticFit[0].get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probeAnnot.loc[logisticFeats, \"Gene.Symbol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we see two probe sets for the same gene (BTG3 in this case)\n",
    "showing up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplot(hessTrain.T.loc[:, \"205548_s_at\"],\n",
    "      hessTrain.T.loc[:, \"213134_x_at\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two are even more tightly correlated than the two probe sets we\n",
    "ran into in the noise modeling excercise above! Let's check their\n",
    "coefficients in the logistic fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsLogisticFit[1].coef_[:, 4:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again despite the highly similar measured expression values\n",
    "associated with the two probe sets, the coefficients take opposite\n",
    "signs! This logistic model fit will likely be improved by\n",
    "regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsRegLogPipeline = pl.Pipeline([\n",
    "    (\"featsel\", fs.SelectKBest(fs.f_regression, k=10)),\n",
    "    (\"classifier\", lm.LogisticRegression(penalty=\"l2\", C=1.0, max_iter=1000))\n",
    "])\n",
    "fsRegLogisticFit = deepcopy(fsRegLogPipeline).fit(hessTrain.T, hessTrainY)\n",
    "fsRegLogisticFit[1].coef_[:, 4:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when L2 regularization is used in fitting logistic regression\n",
    "model, the coefficients for the two probe sets for BTG3 are almost\n",
    "identical (and of much more plausible magnitude as well!). Does\n",
    "regularization effect the test set predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsRegLogisticTestPreds = fsRegLogisticFit.predict_proba(hessTest.T)\n",
    "fsRegLogisticTestPreds[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsRegLogisticTestPredClass = fsRegLogisticFit.predict(hessTest.T)\n",
    "pd.crosstab(fsRegLogisticTestPredClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, regularization produces a model with slightly improved\n",
    "overall accuracy, accurately calling one more pCR and one more RD\n",
    "sample. Beyond this small improvement in estimated model performance,\n",
    "I'd also argue that the regularized model is superior in that the\n",
    "coefficients are more easily interpretable because they do not\n",
    "artificially differentiate between two probe sets for the same\n",
    "underlying gene which show negligible differences in measured\n",
    "expression values.\n",
    "\n",
    "We could also analyze the performance of either or both of the\n",
    "regularized and unregularized feature-selected logistic classification\n",
    "pipelines in the test set using cross validation just as we did with\n",
    "the knn pipelines, but as it doesn't introduce any new concepts I will\n",
    "in the interests of time instead move on.\n",
    "\n",
    "DLDA and Naive Bayes \n",
    "====================\n",
    "\n",
    "\"Naive Bayes\" describes a family of statistical classification\n",
    "methods sharing the common assumption that the feature values are\n",
    "conditionally independent of each other within each class $y$\n",
    "([@lewis1998naive]):\n",
    "$$\\label{eq:naive-bayes-assumption}\n",
    "\\mathbb{P}(\\mathbf{X}=\\mathbf{x} \\mid Y=y) =\n",
    "        \\prod\\limits_g {\\mathbb{P}(X_g=x_g \\mid Y=y)}$$\n",
    "Eq [eq:naive-bayes-assumption](#eq:naive-bayes-assumption) can be substituted into Bayes'\n",
    "formula to calculate classification probabilities:\n",
    "$$\\label{eq:naive-bayes-classification}\n",
    "\\mathbb{P}(Y=y \\mid \\mathbf{X}=\\mathbf{x}) = \\frac{\n",
    "    \\pi_y \\prod\\limits_g {\\mathbb{P}(X_g=x_g \\mid Y=y)}\n",
    "}{\n",
    "    \\sum\\limits_{y'} \\pi_{y'} {\\prod\\limits_g {\\mathbb{P}(X_g=x_g \\mid Y=y')}}\n",
    "}$$\n",
    "\n",
    "where $\\pi_y = \\mathbb{P}(Y=y)$ is the marginal probability (often\n",
    "called a \"prior probability\" in this context) of class $y$ given no\n",
    "information about the feature values $\\mathbf{x}$.\n",
    "\n",
    "Diagonal linear discriminant analysis, or DLDA, is a form of naive\n",
    "Bayes classification with the additional assumption that\n",
    "$\\text{logit}(\\mathbb{P}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})$ is linear in\n",
    "$\\mathbf{x}$, as will be the case if the conditional probability densities\n",
    "for $\\mathbf{X} \\mid Y=0$ and $\\mathbf{X} \\mid Y=1$\n",
    "are both Gaussian with different means but the same (diagonal) covariance\n",
    "([@dudoit2002comparison]). This linearity assumption is\n",
    "shared with logistic regression, though logistic regression generally\n",
    "does *not* make the naive Bayes assumption of Eq\n",
    "[eq:naive-bayes-assumption](#eq:naive-bayes-assumption) and thus usually results in\n",
    "different fit model coefficients.\n",
    "\n",
    "Before we take a look at DLDA itself, let's simplify our data by first\n",
    "extracting the features \"manually\" (this is kosher only because\n",
    "we're not going to do (biased) performance estimation here, just examine\n",
    "the resulting model coefficients!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## extract feature values for selected features to simplify example\n",
    "featData = hessTrain.T.loc[:, logisticFeats]\n",
    " ## center features to 0 mean and scale to unit variance:\n",
    "featData = (featData - featData.mean()) / featData.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's also take a look at what the $t$-statistics\n",
    "that led to these features being selected were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colttests(x, y):\n",
    "    dx = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "    dy = (y - y.mean()) / y.std()\n",
    "    r = np.dot(dy, dx) / (len(y)-1.0)\n",
    "    t = np.sqrt(len(y)-2) * r / (1-r**2)\n",
    "    return pd.Series(t, index=x.columns)\n",
    "\n",
    "tStats = colttests(featData, hessTrainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit a `sklearn.naive_bayes.GaussianNB` model\n",
    "(`sklearn`s version of DLDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "nbFit = nb.GaussianNB().fit(featData, hessTrainY)\n",
    "nbDeltaTheta = nbFit.theta_[1, :] - nbFit.theta_[0, :]\n",
    "gg = qplot(tStats, pd.Series(nbDeltaTheta, index=tStats.index))\n",
    "gg += stat_smooth(method=\"lm\", se=False, size=0.5, color=\"dodgerblue\")\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that: the parameters `nbFit.theta_` of the fit `GaussianNB`\n",
    "model are determined by the $t$-statistics of the corresponding features!\n",
    "(The functional relationship is actually slightly more complicated than\n",
    "the straight line used for the plot, however.)\n",
    "\n",
    "This shouldn't be too surprising: naive Bayes assumes (Eq\n",
    "[eq:naive-bayes-assumption](#eq:naive-bayes-assumption)) that the classifier is determined\n",
    "uniquely by the relationship of each feature *individually* with\n",
    "the class labels. This is exactly the sort of bivariate relationship\n",
    "the $t$-statistic was designed to quantify.\n",
    "\n",
    "Here's one more comparison that might be a bit more suprising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## force a very high value of the regularization parameter lambda:\n",
    "highlyRegLogistic = lm.LogisticRegression(C=1e-4)\\\n",
    "                      .fit(featData, hessTrainY)\n",
    "highlyRegCoef = pd.Series(\n",
    "    highlyRegLogistic.coef_[0, :],\n",
    "    index = featData.columns\n",
    ")\n",
    "gg = qplot(tStats, highlyRegCoef)\n",
    "gg += stat_smooth(method=\"lm\", se=False, size=0.5, color=\"dodgerblue\")\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we see that very highly L2-regularized logistic regression\n",
    "produces linear classifiers whose coefficients are a function of\n",
    "individual feature $t$-statistics, just like DLDA.\n",
    "\n",
    "Naive Bayes: does it work?\n",
    "--------------------------\n",
    "\n",
    "In many cases, yes, naive Bayes (NB) models, including DLDA, work\n",
    "quite well: e.g., the `DLDA30.Value` column is included in\n",
    "`hessTrainAnnot` and `hessTestAnnot` because Hess et al. found\n",
    "that DLDA with 30 features exhibited the best performance under (fancy\n",
    "stratified) cross-validation! More generally, there is a large body of\n",
    "ML literature in which naive Bayes methods have been shown to be\n",
    "surprisingly effective.\n",
    "\n",
    "I say \"surprisingly\" because, outside of artificial simulation\n",
    "settings, the underlying conditional independence assumption is\n",
    "basically never true. So why would might it be effective even when\n",
    "false?\n",
    "1.  We may not have enough data to accurately assess true\n",
    "    inter-feature covariance---there are order $m^2$ pairwise\n",
    "    relationships between features to estimate, as opposed to only $m$\n",
    "    relationships between feature and modeled outcome---so that attempts\n",
    "    to do so just lead to overfitting.\n",
    "2.  While the NB assumption tends to lead to *overconfident*\n",
    "    classifiers---probability scores very near 0 or 1 even when\n",
    "    wrong---it still often leads to *accurate* classifiers---most\n",
    "    calls aren't wrong, even though those that are may be\n",
    "    overconfidently wrong.\n",
    "3.  Counterintuitively, you can show mathematically that NB methods\n",
    "    will result in very accurate (though overconfident) classifiers\n",
    "    assuming that all feature values are in fact *very* strongly\n",
    "    correlated with each other within each class ([@rish2001analysis])!\n",
    "    -   This may be quite relevant in some gene expression studies!\n",
    "    \n",
    "\n",
    "Motivated by the results of Hess et al., let's try a `GaussianNB` model with\n",
    "30 features out on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsNbPipeline = pl.Pipeline([\n",
    "    (\"featsel\", fs.SelectKBest(fs.f_regression, k=30)),\n",
    "    (\"classifier\", nb.GaussianNB())\n",
    "])\n",
    "fsNbFit = deepcopy(fsNbPipeline).fit(hessTrain.T, hessTrainY)\n",
    "nbTestPredClass = fsNbFit.predict(hessTest.T)\n",
    "pd.crosstab(nbTestPredClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 43 out of\n",
    "51 test samples classified correctly. This is\n",
    "slightly better than we did with either logistic regression or knn,\n",
    "but we can't really conclude much from this result since we haven't\n",
    "systematically compared the algorithms using the exact same feature\n",
    "selections or cross-validation folds."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
