{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines \n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine\n",
    "from plotnine import ggplot, aes, geom_line, geom_text, geom_hline, geom_vline\n",
    "import sklearn as sk\n",
    "\n",
    "plotnine.theme_set(plotnine.theme_bw())\n",
    "\n",
    "from ggfuntile import predictionContour\n",
    "from maclearn_utils_2020 import extractPCs, PcaExtractor\n",
    "from load_hess import hessTrain, hessTrainY, hessTest, hessTestY, probeAnnot\n",
    "\n",
    "twoProbeData = hessTrain.T.loc[:, [\"205548_s_at\", \"201976_s_at\"]].copy()\n",
    "twoProbeData.columns =\\\n",
    "        probeAnnot.loc[twoProbeData.columns, \"Gene.Symbol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models, including logistic regression and DLDA, are very useful\n",
    "in many contexts but do have some characteristics which can be\n",
    "limiting. *Support vector machines*, or SVMs\n",
    "([@cortes1995support; @hastie2009elements]), are a type of\n",
    "supervised classifcation algorithm which address two particular\n",
    "limitations:\n",
    "1.  The parameters fit by classical linear classification algorithms\n",
    "    are generally sensitive to extremely easy-to-call sampling units\n",
    "    -   (correctly called sampling units whose feature vectors are\n",
    "        very far from the decision boundary)\n",
    "    -   even when a more accurate classifier might result from\n",
    "        parameters which move these outlier probabilities \"in the wrong\n",
    "        direction\"\n",
    "    -   but not far enough to change the final classification made.\n",
    "    \n",
    "2.  Linearity of response is a very strong and often unrealistic\n",
    "    assumption; many real-world response patterns are highly nonlinear.\n",
    "\n",
    "The term \"support vectors\" in the name SVMs refers to the feature\n",
    "vectors corresponding to samples which are close to being on the wrong\n",
    "side of the decision boundary; for a nice illustration check out\n",
    "<https://en.wikipedia.org/wiki/Support_vector_machine#/media/File:SVM_margin.png>.\n",
    "\n",
    "SVM models are fit by positioning the decision boundary so as to keep\n",
    "the support vectors as far on the right sides as possible; the\n",
    "parameters defining the decision boundary thus ultimately depend only\n",
    "on the sampling units corresponding to the support vectors, thus\n",
    "mitigating point 1 above.\n",
    "\n",
    "Point 2 can also be addressed in the SVM framework using a\n",
    "mathematical technique known as the \"kernel trick.\" The math here is\n",
    "beyond the scope of these notes, but the core idea is that you first\n",
    "apply a nonlinear transformation to the data matrix and then apply SVM\n",
    "in the transformed coordinates, kind of like when we did feature\n",
    "extraction prior to fitting a knn model. The trick is that certain\n",
    "special transformations lead to model fitting problems which may be\n",
    "described in terms of the *un*transformed coordinates in\n",
    "intuitively interesting and useful ways.\n",
    "\n",
    "(What makes this so tricky is that while it has been proven that there\n",
    "do indeed exist particular transformations that lead to the problems\n",
    "nicely describable as modified versions of the original SVM problem in\n",
    "untransformed coordinates, the actual transformations\n",
    "themselves---which are very complicated---aren't actually needed in\n",
    "doing the computations, just the modified problem description in the\n",
    "original feature space!)\n",
    "\n",
    "We'll focus on a specific class of transformations: those which\n",
    "replace the standard dot products appearing in the mathematical\n",
    "expressions composing the original SVM problem with so-called \"radial\n",
    "basis function\" (RBF) kernels.\n",
    "\n",
    "While SVM models are somewhat more complex than the simplicity that is\n",
    "knn, revisiting our two probe set contour plotting strategy using a\n",
    "range of `gamma` parameter values reveals a striking similarity in\n",
    "the types of decision boundaries learned by the methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm\n",
    "svmFitter = svm.SVC(kernel=\"rbf\", C=1, gamma=0.25, probability=True)\n",
    "twoProbeSvmFitSig0p25 = svmFitter.fit(twoProbeData, hessTrainY)\n",
    "predictionContour(twoProbeSvmFitSig0p25,\n",
    "                  twoProbeData, hessTrainY, \"gamma = 0.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the `gamma` parameter creates a more local---and in this\n",
    "case, likely more overfit---SVM model (similar to *decreasing*\n",
    "the number $k$ of nearest neighbors in a knn model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmFitter = svm.SVC(kernel=\"rbf\", gamma=1.25, probability=True)\n",
    "twoProbeSvmFitSig1p25 = svmFitter.fit(twoProbeData, hessTrainY)\n",
    "predictionContour(twoProbeSvmFitSig1p25,\n",
    "                  twoProbeData, hessTrainY, \"gamma = 1.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And increasing `gamma` still further\\..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmFitter = svm.SVC(kernel=\"rbf\", gamma=6.25, probability=True)\n",
    "twoProbeSvmFitSig6p25 = svmFitter.fit(twoProbeData, hessTrainY)\n",
    "predictionContour(twoProbeSvmFitSig6p25,\n",
    "                  twoProbeData, hessTrainY, \"gamma = 6.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SVM models using RBF kernels produce classifiers with somewhat\n",
    "similar properties to knn, you can see that the decision boundaries\n",
    "tend to be smoother. Why might this be?\n",
    "\n",
    "The knn approach doesn't care whether the $(k+1)^{\\text{th}}$ nearest\n",
    "neighbor is just ever so slightly farther away than the\n",
    "$k^{\\text{th}}$, or whether the $k^{\\text{th}}$ nearest neighbor is 10\n",
    "times farther away than the the $(k-1)^{\\text{th}}$, but just gives\n",
    "equal weight to the closest $k$ and zero weight to everything else.\n",
    "\n",
    "In contrast, the SVM-with-RBF-kernel approach can be seen as making\n",
    "predictions using on a *weighted* sum of the known\n",
    "classifications for nearby training data, with the weightings based on\n",
    "a smooth function of the distance from training feature vector to the\n",
    "feature vector to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.pipeline as pl\n",
    "np.random.seed(123)            ## for replicability\n",
    "pcaSvmPipeline = pl.Pipeline([\n",
    "    (\"featextr\", PcaExtractor(m=5)),\n",
    "    (\"classifier\", svm.SVC(kernel=\"rbf\", gamma=\"scale\", probability=True))\n",
    "])\n",
    " ## gamma=\"scale\" uses default gamma value (may not be optimal!)\n",
    "pcaSvmFit = deepcopy(pcaSvmPipeline).fit(hessTrain.T, hessTrainY)\n",
    "pcaSvmTestPredictionClass = pcaSvmFit.predict(hessTest.T)\n",
    "pd.crosstab(pcaSvmTestPredictionClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assess the performance of this classifier in the training set\n",
    "as well, but instead of avoiding resubstitution\n",
    "bias using cross-validation we'll try an alternative resampling\n",
    "technique known as *bootstrapping*.\n",
    "\n",
    "Bootstrapping \n",
    "=============\n",
    "\n",
    "Machine learning is generally less concerned with questions about\n",
    "whether the internal structure of a model is correct, necessary or\n",
    "interpretable than is classical statistics, but there are still times\n",
    "when we'd like to be able to characterize the uncertainty or\n",
    "repeatability associated with an estimated parameter value.\n",
    "\n",
    "Put another way: if we had another data set generated in the same way\n",
    "as the one we do have, how similar would the value we estimated for\n",
    "this or that parameter be to what we get using the actually realized\n",
    "training data? Do we expect to get basically the same value or\n",
    "something wildly different?\n",
    "\n",
    "For linear models, the literature abounds with useful analytical\n",
    "results on confidence intervals, credible intervals, and the like. But\n",
    "for other types of modeling strategies, this is rarely the case!\n",
    "\n",
    "If gathering data were cheap and easy, we could just go ahead and\n",
    "replicate\n",
    "-   the experiment which generated the data and then\n",
    "-   re-fit the model to the newest round of data\n",
    "\n",
    "many times to empirically estimate the distribution of fit model\n",
    "parameters.\n",
    "\n",
    "*Bootstrapping* is a clever approach to *simulate* such\n",
    "replication using just the one data set we actually have\n",
    "([@tibshirani1993introduction]). The bootstrapping process\n",
    "consists of:\n",
    "1.  Generate a case-resampled data set with feature matrix\n",
    "    $\\mathbf{\\underline{X}}^{\\text{boot}}$ and outcome vector\n",
    "    $\\mathbf{y}^{\\text{boot}}$ by drawing $n$ random integers\n",
    "    $1 \\leq r_i \\leq n$ *with replacement* and setting\n",
    "    $$\\begin{aligned}\n",
    "       x_{ig}^{\\text{boot}} &= x_{r_i g} \\\\\n",
    "       y_i^{\\text{boot}} &= y_{r_i}\n",
    "      \\end{aligned}$$\n",
    "    Note that the $r_i$ will generally not be unique: $r_i$ and $r_j$\n",
    "    may be the same sampling unit even when $i \\neq j$, so that the same\n",
    "    sampling unit may be included multiple times in the resampled data\n",
    "    set!\n",
    "2.  Fit desired model to resampled feature matrix\n",
    "    $\\mathbf{\\underline{X}}^{\\text{boot}}$ and outcome vector\n",
    "    $\\mathbf{y}^{\\text{boot}}$ to learn parameters\n",
    "    $\\mathbf{y}^{\\text{boot}}$\n",
    "    -   $\\boldsymbol{\\theta}$ is just way of writing set of all\n",
    "        parameters needed by model pulled together into one big vector,\n",
    "        while\n",
    "    -   the \"hat\" on top of $\\hat{\\boldsymbol{\\theta}}$ indicates\n",
    "        that we are talking about an specific data-derived estimate of the\n",
    "        parameter values $\\boldsymbol{\\theta}$, and\n",
    "    -   superscript \"boot\" on $\\hat{\\boldsymbol{\\theta}}^{\\text{boot}}$\n",
    "        just says the parameters were learned from the bootstrap-resampled\n",
    "        data as opposed to the original trainind data set.\n",
    "    \n",
    "3.  Use fit model with parameters\n",
    "    $\\hat{\\boldsymbol{\\theta}}^{\\text{boot}}$ to estimate parameter or\n",
    "    statistic $\\hat{\\Omega}^{\\text{boot}}$ of interest.\n",
    "4.  Repeat steps 1-3 $B$ times, obtaining values\n",
    "    $\\hat{\\Omega}_b^{\\text{boot}}$ for $b \\in \\{1,\\ldots,B\\}$ using fit models\n",
    "    with parameters $\\hat{\\boldsymbol{\\theta}}_b^\\text{boot}$.\n",
    "\n",
    "Note that because bootstrap resampling generates new simulated data\n",
    "sets of the same size $n$ as the original data set but in which some\n",
    "sampling units are repeated, there will necessarily be some sampling\n",
    "units that get left out in any particular resampled data set: on\n",
    "average, a fraction $\\frac{1}{e} \\approx 0.368$ of all sampling units\n",
    "will be omitted in each bootstrap sample.\n",
    "\n",
    "Bootstrapping for Performance Estimation \n",
    "----------------------------------------\n",
    "\n",
    "Bootstrapping can also be used as an alternative to cross-validation\n",
    "for estimation of prediction error $\\Omega$.\n",
    "\n",
    "How should we go about this?\n",
    "-   We might try to estimate distribution of prediction error\n",
    "    $\\{\\hat{\\Omega}_b^{\\text{full}}\\}$\n",
    "-   making predictions with each bootstrap model $b$ with parameters\n",
    "    $\\hat{\\boldsymbol{\\theta}}_b^\\text{boot}$ applied to full (original)\n",
    "    training set $\\mathbf{\\underline{X}}$.\n",
    "\n",
    "However, since bootstrap training sets were drawn from the same\n",
    "original feature matrix $\\mathbf{\\underline{X}}$,\n",
    "$\\{\\hat{\\Omega}_b^{\\text{full}}\\}$ will suffer from resubstitution\n",
    "bias.\n",
    "\n",
    "Instead we could follow cross-validation methodology:\n",
    "-   use only fit models with parameters $\\hat{\\boldsymbol{\\theta}}_b$ for\n",
    "    which\n",
    "-   sampling unit $i$ not used in the $b^{\\text{th}}$ resampled\n",
    "    training set.\n",
    "\n",
    "Writing $R_b$ to indicate the set of sampling units included in the\n",
    "$b^{\\text{th}}$ resampled training set:\n",
    "$$\\label{eq:loo-boot}\n",
    "\\hat{\\Omega}^{\\text{loo-boot}} = \\frac{1}{n} \\sum\\limits_{i} {\n",
    "    \\frac{1}{|\\{b \\mid i \\notin R_b \\}|}\n",
    "    \\sum\\limits_{\\{b \\mid i \\notin R_b \\}} \\hat{\\Omega}(\\hat{\\boldsymbol{\\theta}}_b, y_i)\n",
    "}$$\n",
    "(Aside re: set notation: $\\{b \\mid i \\notin R_b \\}$ is set of\n",
    "bootstrap iterations $b$ for which sampling unit $i$ does not appear\n",
    "in the set of sampling units $R_b$, while $|{b i R_b }|$\n",
    "is the number of elements in this set, that is, the number of\n",
    "bootstrap iterations which omitted sampling unit $i$.)\n",
    "\n",
    "But while $\\{\\hat{\\Omega}_b^{\\text{full}}\\}$ are generally overly\n",
    "optimistic, $\\hat{\\Omega}^{\\text{loo-boot}}$ may be too\n",
    "*pessimistic*, since each bootstrap case-resampled training set\n",
    "generally contains only a fraction $1-\\frac{1}{e} \\approx 0.632$ of\n",
    "the true training sampling units (albeit with some showing up multiple\n",
    "times!).\n",
    "\n",
    "Since repeating training sampling units doesn't generally improve\n",
    "models---the repeated units aren't really new data!---we are\n",
    "effectively learning models using only $\\approx 63.2%$ of the\n",
    "available data (albeit randomly upweighting some sampling units\n",
    "relative to others).\n",
    "[@efron1997improvements] showed that\n",
    "$$\\label{eq:632-bootstrap}\n",
    "\\hat{\\Omega}^{.632} = 0.368 \\, \\hat{\\Omega}^{\\text{resub}} + 0.632 \\, \\hat{\\Omega}^{\\text{loo-boot}}$$\n",
    "strikes a good balance between the optimism of $\\hat{\\Omega}^{\\text{resub}}$\n",
    "and the pessimism of $\\hat{\\Omega}^{\\text{loo-boot}}$ in some situations.\n",
    "\n",
    "However, in cases where overfitting is more severe,\n",
    "[@efron1997improvements] recommend\n",
    "$$\\label{eq:632plus-bootstrap}\n",
    "\\hat{\\Omega}^{.632+} =\n",
    "(1-\\hat{w}) \\, \\hat{\\Omega}^{\\text{resub}} +\n",
    "\\hat{w} \\, \\hat{\\Omega}^{\\text{loo-boot}}$$\n",
    "where $\\hat{w} \\in [1-\\frac{1}{e}, 1]$ depends on the degree of\n",
    "overfitting.\n",
    "\n",
    "There is a standard formula for calculating $\\hat{w}$ for estimating\n",
    "prediction error using the .632+ bootstrap which you can look up;\n",
    "aside from [@efron1997improvements], [@hastie2009elements] has\n",
    "a nice treatment.\n",
    "\n",
    "OK, let's get back to a concrete example: we'll use bootstrapping to\n",
    "assess the performance of a select-10-feature-for-SVM-modeling\n",
    "pipeline using 25 bootstrap resamples (this is a relatively low number\n",
    "for illustration purposes only; most sources suggest $100$\n",
    "resamples with bootstrapping, but that takes a while!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## bootstrap .632+ implemented in mlxtend module:\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "import sklearn.feature_selection as fs \n",
    "fsSvmPipeline = pl.Pipeline([\n",
    "    (\"featsel\", fs.SelectKBest(fs.f_regression, k=10)),\n",
    "    (\"classifier\", svm.SVC(kernel=\"rbf\", gamma=\"scale\", probability=True))\n",
    "])\n",
    "np.random.seed(321)\n",
    "fsSvmBootAccs = bootstrap_point632_score(\n",
    "    estimator = fsSvmPipeline,\n",
    "    X = hessTrain.T.values,  ## .values b/c mlxtend likes numpy arrays\n",
    "    y = hessTrainY.values,   ## .values b/c mlxtend likes numpy arrays\n",
    "    method = \".632+\",\n",
    "     ## here do only 25 bootstrap resamples for speed;\n",
    "     ## (usually recommended to do >= 100 in real usage!)    \n",
    "    n_splits = 25\n",
    ")\n",
    "np.mean(fsSvmBootAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifiers\n",
    "=========================\n",
    "\n",
    "Decision trees are probably understood by considering an example. A\n",
    "single decision tree can be constructed in Python using the function\n",
    "`sklearn.tree.DecisionTreeClassifier`. \n",
    "\n",
    "The standard process of fitting a decision trees, which is sometimes\n",
    "referred to as \"recursive partitioning\", actually performs a\n",
    "form of embedded feature selection, but to keep these notes\n",
    "as similar to the R version as possible, we'll connect our by now old-hat\n",
    "$t$-test feature selector upstream of `tree.DecisionTreeClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "decTreePipeline = pl.Pipeline([\n",
    "    (\"featsel\", fs.SelectKBest(fs.f_regression, k=100)),\n",
    "    (\"classifier\", tree.DecisionTreeClassifier(\n",
    "        min_samples_split = 10,  ## don't split if < 10 sampling units in bin\n",
    "        max_depth = 3            ## split splits of splits but no more!\n",
    "    ))\n",
    "])\n",
    "fsDecTree = decTreePipeline.fit(hessTrain.T, hessTrainY)\n",
    "tree.plot_tree(fsDecTree[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node of the tree shown is associated with a subset of the set of\n",
    "all sampling units. The topmost (or root) node contains all samples,\n",
    "which are then split (or partitioned) into those samples for which the\n",
    "expression level for probe set 212745_s_at was measured to be \\<\n",
    "7.673, which flow down to the left node, and those samples with higher\n",
    "levels of 212745_s_at expression, which go down the right\n",
    "branch. The fitting algorithm selected the probe set 212745_s_at and\n",
    "the level 7.673 for the top split because this was determined to be\n",
    "the best single split to separate pCR patient samples from RD patient\n",
    "samples.\n",
    "\n",
    "The \"recursive\" part of recursive partitioning is then to repeat\n",
    "this splitting process within each of those sample subpopulations,\n",
    "*unless* one of the stopping criteria is met. Stopping criteria\n",
    "are usually based on the size and \"impurity\" of the sample\n",
    "subpopulation: If the node is associated with too small a sample\n",
    "subpopulation it will not be split, or if the sample subpopulation\n",
    "within the node is sufficiently pure in either one outcome class or\n",
    "the other (either close to all pCR or close to all RD), there is no\n",
    "point in further splitting.\n",
    "\n",
    "Classification probabilities for any new sample may then be calculated\n",
    "by starting at the root and following the branches of the tree\n",
    "indicated the sample's feature values until a terminal, or leaf, node\n",
    "is reached: the fraction of training set samples in the leaf node with\n",
    "classification RD is then the predicted probability that patient from\n",
    "which the new sample is derived will suffer from residual invasive\n",
    "disease (RD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsDecTreePredProbs = fsDecTree.predict_proba(hessTest.T)\n",
    "fsDecTreePredProbs[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsDecTreePredClass = fsDecTree.predict(hessTest.T)\n",
    "pd.crosstab(fsDecTreePredClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single decision trees are simple and intuitive but, despite the\n",
    "reasonably good results seen just above, have generally not performed\n",
    "very well in real world classification tasks. The structure of such\n",
    "trees also tends to be very sensitive to small changes in the training\n",
    "data; don't be surprised if you get an entirely different tree if a\n",
    "single sampling unit is added or removed from the training data set!\n",
    "\n",
    "There is, however, an approach to machine learning based on multiple\n",
    "decision trees which has become very popular in the last few\n",
    "decades\\...\n",
    "\n",
    "Bagging: **Bootstrap** **Agg**regat**ing** Models \n",
    "=================================================\n",
    "\n",
    "We could consider using set of $B$ bootstrap case-resample trained\n",
    "models in place of a single model for making predictions.\n",
    "Repeat for $b {1,...,B}$:\n",
    "1.  Generate $\\mathbf{\\underline{X}}_b$ by drawing $n$\n",
    "    random integers $R_b=\\{r_{b i}\\}$ with replacement\n",
    "    and setting $x_{b ig} = x_{r_{b i} g}$, $y_{b i} = y_{r_{b i}}$.\n",
    "2.  Fit model using $\\mathbf{\\underline{X}}_b$ and $\\mathbf{y}_b$ to\n",
    "    obtain fitted parameters $\\hat{\\boldsymbol{\\theta}}_b$.\n",
    "\n",
    "Bagged predictions for new datum with feature vector $\\mathbf{x}$ by\n",
    "simply averaging together the predictions of each bagged submodel $b$\n",
    "with parameters $\\hat{\\boldsymbol{\\theta}}_b$ for features $\\mathbf{x}$.\n",
    "From [@breiman1996bagging]:\n",
    "> For unstable procedures bagging works well ...The evidence,\n",
    "> both experimental and theoretical, is that bagging can push a\n",
    "> good but unstable procedure a significant step towards optimality.\n",
    "> On the other hand, it can slightly degrade the performance of stable\n",
    "> procedures.\n",
    "\n",
    "In this context, \"stability\" is of the fit model parameters\n",
    "$\\boldsymbol{\\theta}$ with respect to the training data\n",
    "$\\{\\mathbf{x}_i, y_i\\}$. Recall that I said in section\n",
    "[sec:decision-trees](#Decision-Tree-Classifiers) that decision trees suffered\n",
    "from exactly this sort of instability!\n",
    "\n",
    "In fact the most well-known application of bagging is indeed the\n",
    "generation of *random forests* of decision trees\n",
    "([@breiman1999random]). A random forest is constructed by\n",
    "repeating, for $b {1,...,B}$:\n",
    "1.  Generate $\\mathbf{\\underline{X}}_b$ and $\\mathbf{y}_b$ by drawing $n$ random\n",
    "    integers $R_b=\\{1 \\leq r_{b i} \\leq n\\}$ with replacement and setting\n",
    "    $x_{b ig} = x_{r_{b i} g}$ and $y_{b i} = y_{r_{b i}}$.\n",
    "2.  Randomly select $m' < m$ of the features and fit a decision tree\n",
    "    classifier for $\\mathbf{y}_b$ using the columns of feature matrix\n",
    "    $\\mathbf{\\underline{X}}_b$ corresponding to those features.\n",
    "    -   $m'$ random features redrawn for each new split.\n",
    "    -   Commonly $m' \\approx \\sqrt{m}$.\n",
    "    \n",
    "`sklearn.ensemble` includes a class `RandomForestClassifier`\n",
    "which is quite easy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ens\n",
    "np.random.seed(321)\n",
    "rf = ens.RandomForestClassifier(\n",
    "    n_estimators = 100,          ## number of trees\n",
    "    min_samples_split = 10\n",
    ").fit(hessTrain.T, hessTrainY)\n",
    "\n",
    "rfPredClass = rf.predict(hessTest.T)\n",
    "pd.crosstab(rfPredClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So\\...here we found that a single decision tree combined with upstream\n",
    "simple $t$-test feature selection of 100 probe sets outperformed a\n",
    "random forest of 100 trees. Don't think this is a typical\n",
    "result---random forests have been found to generate very competitive\n",
    "ML classifiers in a wide variety of situations, while single decision\n",
    "trees generally have not. But it does go to show that it can be hard\n",
    "to generalize about ML algorithm performance, especially on relatively\n",
    "small data sets like the Hess example here!\n",
    "\n",
    "Classification Performance Metrics \n",
    "==================================\n",
    "\n",
    "There are many ways to measure performance for classifiers, of which\n",
    "accuracy is only one. Like accuracy, most are based the discrete\n",
    "classification label calls. For classifiers which output probability\n",
    "scores, this means that some threshold probability $\\psi$ (often, but\n",
    "certainly not always, 0.5) must be set.\n",
    "\n",
    "For binary (two-class) classification, when one class can be\n",
    "considered \"positive\" and the other \"negative, the cells of the 2x2\n",
    "contingency table are often labeled as true positive (TP), true\n",
    "negative (TN), false positive (FP), and false negative (FN), where,\n",
    "e.g., a false positive is sampling unit which the classifier declares\n",
    "positive but for which the true value of the outcome is negative.\n",
    "\n",
    "We could consider the values of such hard-call metrics over range of\n",
    "threshold values $\\psi$. The so-called receiver operating\n",
    "characteristic (ROC) curve ([@fawcett2006introduction]) does this\n",
    "for sensitivity and specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## pick 20 test samples to score with pcaSvmFit classifier:\n",
    "np.random.seed(123)\n",
    "xfew = hessTrain.iloc[:, np.random.permutation(hessTrain.shape[1])[0:20]].T\n",
    "yis1 = hessTrainY.loc[xfew.index] == 1\n",
    " ## do the scoring:\n",
    "fewPredProbs = pcaSvmFit.predict_proba(xfew)[:, 1]\n",
    "fewPredProbs = pd.Series(fewPredProbs, index=xfew.index)\n",
    " ## set up vector all threshold values at which a call would change:\n",
    "thresholds = pd.concat([\n",
    "    pd.Series([1.0], index=[\"none\"]),\n",
    "    fewPredProbs.sort_values(ascending=False),\n",
    "    pd.Series([0.0], index=[\"all\"])\n",
    "])\n",
    " ## calculate number true positives at each threshold:\n",
    "tp = pd.Series(\n",
    "    [np.sum((fewPredProbs > thresh) & yis1) for thresh in thresholds],\n",
    "    index = thresholds.index\n",
    ")\n",
    " ## and also number true negatives at each threshold:\n",
    "tn = pd.Series(\n",
    "    [np.sum((fewPredProbs <= thresh) & ~yis1) for thresh in thresholds],\n",
    "    index = thresholds.index\n",
    ")\n",
    " ## scale these by totals to obtain sens, spec at each threshold value:\n",
    "sensitivity = tp / np.sum(yis1)\n",
    "specificity = tn / np.sum(~yis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated sensitivity and specificity at every meaningful\n",
    "threshold value, we can now plot the ROC curve using `ggplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggdata = pd.DataFrame({\n",
    "    \"sample\" : sensitivity.index,\n",
    "    \"actual_class\" : yis1.reindex(sensitivity.index).astype(float).values,\n",
    "    \"score\" : fewPredProbs.reindex(sensitivity.index).values,\n",
    "    \"sensitivity\" : sensitivity,\n",
    "    \"specificity\" : specificity\n",
    "})\n",
    "ggdata[\"1-specificity\"] = 1 - ggdata[\"specificity\"]\n",
    "gg = ggplot(ggdata, aes(x=\"1-specificity\", y=\"sensitivity\"))\n",
    "gg += geom_line(aes(color=\"score\"), size=1, alpha=0.75)\n",
    "gg += geom_text(mapping = aes(label=\"sample\"),\n",
    "                data = ggdata.loc[ggdata[\"actual_class\"] == 1, :],\n",
    "                color = \"red\")\n",
    "gg += geom_text(mapping = aes(label=\"sample\"),\n",
    "                data = ggdata.loc[ggdata[\"actual_class\"] == 0, :],\n",
    "                angle = -90,\n",
    "                color = \"black\")\n",
    "gg += geom_hline(mapping = aes(yintercept=\"sensitivity\"),\n",
    "                 data = ggdata.loc[ggdata[\"actual_class\"] == 1, :],\n",
    "                 alpha = 0.35,\n",
    "                 size = 0.25)\n",
    "gg += geom_vline(mapping = aes(xintercept=\"1-specificity\"),\n",
    "                 data = ggdata.loc[ggdata[\"actual_class\"] == 0, :],\n",
    "                 alpha = 0.35,\n",
    "                 size = 0.25)\n",
    "gg += plotnine.scale_color_gradientn(\n",
    "    colors = [\"orangered\", \"goldenrod\", \"seagreen\", \"dodgerblue\", \"#606060\"]\n",
    ")\n",
    "gg += plotnine.theme_classic()\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in this plot that there are 15 RD (or\n",
    "\"positive\") and 5 pCR (\"negative\") samples in the\n",
    "subampled test data `xfew`: 3 of the\n",
    "5 negative samples---M153, M206, M111---have\n",
    "lower prediction scores than any of the 15 positive samples.\n",
    "Thus, there are 3 times 15 =\n",
    "45 light gray vertices below the\n",
    "ROC curve in the 3 columns on the right of\n",
    "the plot.\n",
    "\n",
    "Adding to this the 26 light gray verices below the ROC curve along the\n",
    "vertical lines labeled by samples M125 and M309, corresponding to the 26\n",
    "positive samples with scores higher than that of the negative samples\n",
    "M125 and M309, we obtain 71 total ways of pairing one of the positive\n",
    "samples with one of the negative samples for which the positive sample\n",
    "has a higher score than the negative.\n",
    "\n",
    "This corresponds to a fraction of 71 out of the 75, or 0.9467, vertices in the\n",
    "plot which lie below the curve. This shows that the area under the curve (AUC)\n",
    "for the ROC curve is 0.9467,\n",
    "which must also be the likelihood that if we randomly pick one positive sample\n",
    "and one negative sample from these 20 the positive sample will have a higher\n",
    "score than the negative.\n",
    "\n",
    "The ROC AUC score is one the most popular metrics for assessing\n",
    "classifier performance. Beyond being threshold-independent---since it\n",
    "aggregates over all possible thresholds by considering the full ROC\n",
    "curve---it has the property that an uninformative classifier will have\n",
    "an AUC of 0.5 even when the two classes are unbalanced (more of one\n",
    "than the other), as they are in the Hess data (almost 3x as many RD as\n",
    "pCR).\n",
    "\n",
    "This is not the case with accuracy: if you just assign all sampling\n",
    "units the same classification score (ignoring all feature values) and\n",
    "then set the classification threshold so that they are all called the\n",
    "more common class, the accuracy will be the > 0.5 fraction assigned to\n",
    "that class (almost 0.75 in the case of the Hess set!).\n",
    "\n",
    "We don't usually want to do all of the work we did above to assess the\n",
    "AUC score for a classifier; here are two easier ways to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## calculate ROC-AUC using sklearn.metrics.roc_auc_score\n",
    "import sklearn.metrics as met\n",
    "met.roc_auc_score(yis1, fewPredProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## or can calculate from scipy.stats.mannwhitneyu statistic:\n",
    "import scipy.stats as stats\n",
    " ## (this nonparametric test is based on same underlying information):\n",
    "mwu = stats.mannwhitneyu(fewPredProbs[yis1],\n",
    "                         fewPredProbs[~yis1],\n",
    "                         alternative = \"less\")\n",
    "# MannwhitneyuResult(statistic=70.0, pvalue=0.9980146250417405)\n",
    "mwu[0] / (np.sum(yis1) * np.sum(~yis1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In other words, the ROC AUC score is essentially a more interpretable\n",
    "rescaling of the Wilcoxon-Mann-Whitney test (also known as the\n",
    "Mann-Whitney U test) statistic. This makes sense in light of the\n",
    "intepretation of AUC as the chance that a randomly chosen positive\n",
    "case has a higher classification score than does a randomly chosen\n",
    "negative case, since the Wilcoxon-Mann-Whitney test is based on this\n",
    "same idea.)\n",
    "\n",
    "Of course, we can get better estimate of the AUC using the\n",
    "*whole* test set instead of just `xfew`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaSvmTestPredProbs = pcaSvmFit.predict_proba(hessTest.T)\n",
    "met.roc_auc_score(hessTestY, pcaSvmTestPredProbs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a bit worse---but this still shows thus that even though\n",
    "`pcaSvmFit` only managed to correctly call 2 of the 13 test pCR\n",
    "samples as negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaSvmTestPredictionClass = pcaSvmFit.predict(hessTest.T)\n",
    "pd.crosstab(pcaSvmTestPredictionClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the scores of the negative (pCR) samples still tend to be lower than\n",
    "the scores of the positive (RD) samples, even if they are above the\n",
    "default threshold $\\psi=0.5$.\n",
    "\n",
    "Wrap-up: Comparing Models by AUC \n",
    "--------------------------------\n",
    "\n",
    "Let's go back and try a quick head-to-head comparison of five of the\n",
    "different classification models we've covered. First let's make sure\n",
    "we have all of the necessary libraries loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as nbr    ## KNeighborsClassifier\n",
    "import sklearn.linear_model as lm  ## LogisticRegression\n",
    "import sklearn.svm as svm          ## SVC\n",
    "import sklearn.naive_bayes as nb   ## GaussianNB\n",
    "import sklearn.ensemble as ens     ## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a dictionary comprehension to loop through the\n",
    "five different classification\n",
    "strategies. This method wants to be supplied a `dict` to work with,\n",
    "and will output a `dict` of result objects with the same keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstreamFitters = {\n",
    "    \"knn\" : nbr.KNeighborsClassifier(n_neighbors=9),\n",
    "    \"l2logistic\" : lm.LogisticRegression(penalty=\"l2\", max_iter=1000),\n",
    "    \"nb\" : nb.GaussianNB(),\n",
    "    \"svm\" : svm.SVC(kernel=\"rbf\", C=1, gamma=\"scale\", probability=True),\n",
    "    \"randomForest\" : ens.RandomForestClassifier(n_estimators=500,\n",
    "                                                min_samples_split=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to hook up a common upstream feature selection\n",
    "strategy (guess which one we'll use!) to each of the\n",
    "`downstreamFitters`, fit the resulting pipeline, make predictions on\n",
    "the test set, and calculate the resulting AUC scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    " ## use dictionary comprehension to loop through all downstreamFitters:\n",
    "fitModels = {name : pl.Pipeline([\n",
    "                 (\"featsel\", fs.SelectKBest(fs.f_regression, k=30)),\n",
    "                 (\"classifier\", deepcopy(downstreamFitter))\n",
    "             ]).fit(hessTrain.T, hessTrainY)\n",
    "             for name, downstreamFitter in downstreamFitters.items()}\n",
    " ## now loop through fitModels to predict probabilities:\n",
    "fitPredProbs = {name : fitModel.predict_proba(hessTest.T)[:, 1]\n",
    "                for name, fitModel in fitModels.items()}\n",
    " ## and finally loop through fitPredProbs to get AUC values:\n",
    "fsAucs = {name : met.roc_auc_score(hessTestY, predProbs)\n",
    "          for name, predProbs in fitPredProbs.items()}\n",
    "fsAucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that the simplest strategy, knn, ends up winning\n",
    "according to this comparison! Lots of caveats here: the results might\n",
    "look very different with different methods of feature selection or\n",
    "extraction, different numbers of features retained, different settings\n",
    "of the various modeling parameters (number of nearest neighbors, SVM\n",
    "cost or sigma parameters, number of trees in random forests, etc.), so\n",
    "I wouldn't advise reading too much into this beyond this: sometimes\n",
    "simplicity works."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
