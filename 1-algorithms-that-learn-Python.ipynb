{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms That Learn To Predict \n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning (ML) refers to the use of algorithms which can learn\n",
    "from data. The inputs to a ML algorithm will generally be some sort of\n",
    "data set---which I will refer to as the *training data*---and the\n",
    "output will usually be another algorithm, which I will call a\n",
    "*fit model*, or sometimes just a *fit* if I'm feeling lazy.\n",
    "-   ML algorithm : training data $\\rightarrow$ fit model\n",
    "\n",
    "The fit model itself also takes data as input, and generally requires\n",
    "that the data provided to it must be very similar in nature to that\n",
    "provided to the ML algorithm as training data: For example, assuming\n",
    "the data sets in question are represented in table form, the data\n",
    "provided to the fit model must usually have all or almost all of the\n",
    "same columns as the training data set did. However, the output from\n",
    "the fit model is usually much simpler, often consisting of a predicted\n",
    "*numeric value* or *categorical label* for each individual\n",
    "sampling unit of the data set.\n",
    "-   Fit model : test data $\\rightarrow$ predicted values\n",
    "\n",
    "We will use the `sklearn` python module, which implements ML\n",
    "algorithms in an object-oriented manner. In this framework,\n",
    "a constructor is used to create a learner object embodying the algorithm;\n",
    "the `fit` method can then be called on the learner object, after\n",
    "which it becomes the fit model object. I sometimes `deepcopy` the\n",
    "learner and fit the copy so as to keep the objects representing\n",
    "the learner and fit model separate; this can be useful if you want\n",
    "to re-use the same learner to fit multiple fit models using different\n",
    "data sets.\n",
    "\n",
    "Here is an example using `sklearn.linear_model.LinearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n = 20\n",
    " ## generate some random data for two variables:\n",
    "predictor1 = np.random.randn(n)\n",
    "predictor2 = np.random.randn(n)\n",
    " ## now set a third variable equal to a weighted sum of those\n",
    " ## two variables plus a random error term:\n",
    "output = 2*predictor1 + predictor2 + np.random.randn(n)\n",
    " ## bundle up the three variables composing our data set into a\n",
    " ## DataFrame object:\n",
    "featureData = pd.DataFrame({\"p1\":predictor1, \"p2\":predictor2})\n",
    " ## split featureData and output into training and test sets:\n",
    "trainFeats = featureData.iloc[0:10, ]\n",
    "trainOutput = output[0:10]\n",
    "testFeats = featureData.iloc[10:20, ]  ## should not overlap trainData!\n",
    "testOutput = output[10:20]\n",
    " ## now train model using only trainData:\n",
    "import sklearn.linear_model as lm\n",
    "learner = lm.LinearRegression()\n",
    "fitModel = deepcopy(learner).fit(trainFeats, trainOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `fitModel` to make predictions on rows 11--20 of\n",
    "`featData`; in `sklearn`, this is done by calling the `predict`\n",
    "method of `fitModel` with the the test feature matrix as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## generate predictions for test data:\n",
    "predictions = fitModel.predict(testFeats)\n",
    " ## we'll use plotnine (python version of ggplot) for plotting:\n",
    "import plotnine\n",
    "from plotnine import qplot, ggplot, aes, geom_point\n",
    " ## get rid of default gray background:\n",
    "plotnine.theme_set(plotnine.theme_bw())\n",
    " ## plot actual values of out column against predicted values\n",
    " ## for the test data using ggplot2::qplot\n",
    "qplot(pd.Series(predictions), pd.Series(testOutput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This an example of *supervised learning*, in which one of the\n",
    "variables in the training data set (`out` in this case) is treated\n",
    "as an output to be predicted using the others. Note that the test set\n",
    "does not need to have this variable present to make predictions;\n",
    "indeed we did not give it that information in the `predict` call\n",
    "above!\n",
    "\n",
    "Thus, in supervised learning approaches the fit model requires only a\n",
    "subset of the variables present in the training data to be present in\n",
    "the test data in order to make predictions.\n",
    "\n",
    "In *unsupervised learning* this is not the case, and we must\n",
    "generally have all variables from the training data also present in\n",
    "any test data that we wish to make predictions on. What is this\n",
    "\"unsupervised learning\", you ask, and what might it be used to\n",
    "predict? Let's consider an example to make things more concrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainFeats.copy()\n",
    "trainData[\"out\"] = trainOutput\n",
    "testData = testFeats.copy()\n",
    "testData[\"out\"] = testOutput\n",
    " ## use k-means clustering algorithm to fit 2 clusters to training data\n",
    "import sklearn.cluster as clust\n",
    "kmLearner = clust.KMeans(n_clusters=2)\n",
    "kmeansFit = deepcopy(kmLearner).fit(trainData)\n",
    " ## predict which cluster each test datum is in:\n",
    "kmPredictions = kmeansFit.predict(testData)\n",
    "kmPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## two clusters in this case correspond to low and high values of \"out\":\n",
    "qplot(pd.Series(kmPredictions).astype(str), pd.Series(testOutput), geom=\"boxplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in this example, unsupervised learning algorithms try to find\n",
    "some latent structure in the training data---such as the carving of\n",
    "the variable space (frequently called *feature space* in ML) into\n",
    "two disjoint clusters done by `kmeans`, about which more will be\n",
    "said later.\n",
    "\n",
    "Many unsupervised learning algorithms, including `kmeans`, produce\n",
    "fit models which can be used to determine how test data would fit into\n",
    "the learned latent structure; for instance, here we were able to\n",
    "assign each test datum to one of the two clusters learned from the\n",
    "training data set. There are some unsupervised learning approaches\n",
    "which generate fit models which are not immediately equipped to make\n",
    "test set predictions, however---hierarchical clustering and tSNE come\n",
    "to mind here---which can limit their utility in some situations.\n",
    "\n",
    "Data \n",
    "====\n",
    "Machine learning---perhaps I should lose the qualifier and just say\n",
    "learning---isn't much without data!\n",
    "\n",
    "We're going to see how machine learning algorithms work by applying\n",
    "them to both real and simulated data. It's critical to play with real\n",
    "data in learning machine learning, as it is very difficult to\n",
    "replicate many important features of real data via\n",
    "simulation. Simulation does play an important role in ML as well,\n",
    "however: only with simulated data can we check how our algorithms\n",
    "perform when all of the assumptions that underlie their derivation are\n",
    "truly met. It is also frequently much easier to \"turn the knobs\" on\n",
    "various data set properties of interest---like the number of sampling\n",
    "units $n$, the number of features $m$, the degree of correlation\n",
    "between features, etc.---with simulated data than in the lab or the\n",
    "external world!\n",
    "\n",
    "We will consider two real gene expression data sets:\n",
    "1.  an RNA-seq data set downloaded from Gene Expression Omnibus\n",
    "    (accession\n",
    "    [GSE120430](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE120430))\n",
    "    analyzing transcriptional targets of core promoter factors in\n",
    "    Drosophila neural stem cells [@neves2019distinct].\n",
    "2.  a microarray (!) data set from 2006 collected to predict\n",
    "    sensitivity to preoperative chemotherapy using expression levels\n",
    "    measured in fine-needle breast cancer biopsy specimens\n",
    "    [@hess2006pharmacogenomic].\n",
    "\n",
    "I'll defer further discussion of the Hess data set until we get to\n",
    "supervised analysis methods.\n",
    "\n",
    "In order to read in the data from file, I'm going to define a\n",
    "convenience function resetting some of the defaults of the\n",
    "`pd.read_csv` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt(f):\n",
    "    return pd.read_csv(f, sep=\"\\t\", index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this function to read the Neves data set, along with a\n",
    "file containing Drosophila melanogaster gene annotations, in from the\n",
    "files included here in the github project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevesExpr = np.log2(rt(\"data/gse120430_deseq_normalized.tsv.gz\") + 1)\n",
    "nevesExpr.iloc[0:5, 0:5]\n",
    " ## (note that gene expression matrix files are usually provided\n",
    " ##  using genes-in-rows format)\n",
    " ## simplify nevesExpr by removing genes with no data:\n",
    "nevesExpr = nevesExpr.loc[nevesExpr.sum(axis=1) > 0]\n",
    " ## by contrast, sample annotation files generally follow the\n",
    " ## older statistics convention of sampling units-in-rows\n",
    "nevesAnnot = rt(\"data/gse120430_sample_annotation.tsv\")\n",
    "dmGenes = rt(\"data/d_melanogaster_gene_annotations.saf.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at `nevesAnnot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevesAnnot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the chance of any bugs in our analysis code, it is useful\n",
    "to align the rows of the sample annotation data (and gene annotation\n",
    "data, if we have it) to the columns of the expression matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## align sample annotations to expression data:\n",
    "nevesAnnot = nevesAnnot.loc[nevesExpr.columns]\n",
    " ## align dmGenes to expression data:\n",
    "dmGenes = dmGenes.loc[nevesExpr.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `group` column indicates whether each sample is in group\n",
    "expressing the control (mCherry) or one of the experimental RNAi\n",
    "transgenes (TAF9, TBP, or TRF2).\n",
    "The sample names in the expression data and sample annotations are\n",
    "Gene Expression Omnibus accession ids; we'll replace these with more\n",
    "descriptive names based on the grouping information in the sample\n",
    "annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## use more descriptive names for samples\n",
    "betterSampleNames = [nevesAnnot[\"group\"].iloc[i] + \"-\" + str(1+i%3)\n",
    "                     for i in range(nevesAnnot.shape[0])]\n",
    "nevesExpr.columns = betterSampleNames\n",
    "nevesAnnot.index = betterSampleNames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, because the descriptive gene names for the measured\n",
    "Drosophila genes are in one-to-one correspondence with the Flybase\n",
    "gene ids used to label the rows in the file\n",
    "`data/gse120430_deseq_normalized.tsv.gz`, we'll swap them\n",
    "out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## use more descriptive names for genes\n",
    "nevesExpr.index = dmGenes[\"GeneName\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code shown above for loading in the Neves data set is also contained in the file `load_neves.py`."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
